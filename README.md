# CCP-AWS-CLFC01

# Summary of all AWS services  
![aws drawio](https://github.com/souravs17031999/CCP-AWS-CLFC01/assets/33771969/d0196c30-db48-48da-bed2-890037b2fadb)    


# CLOUD CONCEPTS   

## CASE STUDY For organizations moving to cloud from on-premise and cloud value framework:  

 Key Findings
– A survey of companies spending $100,000 or more annually on public cloud shows that 80%
exceed their cloud budgets in most months. Yet 95% of respondents agree that using cloud
services reduces the TCO for IT infrastructure when compared with on-premises equivalents.
- Unit costs for public cloud infrastructure have stayed low even as macroeconomic inflation has
reached historic highs. Our research finds that cloud buyers are spending the money they save by
moving to cloud on new services to improve productivity and derive new revenue, not necessarily
to reduce costs.
- Businesses implementing cloud financial management practices report that their efforts are most
likely to result in greater cloud adoption, higher revenue and improved profitability.
- Based on the results of our survey, best practices for cloud cost control include: using the full
range of available services (infrastructure, platform and software); taking advantage of the
full range of pricing models available (on-demand, reserved instances, savings plans and spot
instances); allocating costs in a way that is meaningful in business terms, whether via account
management, resource tagging or chargeback/showback; and ensuring that financial management
is in the hands of teams with both ownership and accountability for cloud spending.
- IT teams are the primary drivers of cloud financial discipline for more than half of organizations,
but this responsibility can change depending on the degree of spending, with the duty shifting to
platform and finance teams at higher levels.
- IT teams are also very involved in setting sustainability goals: 86% of the organizations in our
sample have a formal sustainability program in place; of those, 71% report that IT is very involved in
setting and meeting sustainability goals.

## Figure 5: Monthly forecasting is most common  
## For what time period do you forecast your cloud spending?
Base: All respondents (n=959).
Source: 451 Research custom survey, 2022.
“[Our ability to manage and control cloud costs] is pretty good because we reexamine it every month. 
We produce these cost optimizations. We produce capacity reports and utilization reports. 
So I think you control it by providing the data, cost, performance, capacity availability. And if you provide the data, then
make it digestible by the managers and the finance people, then it drives the behavior.”
IT/engineering manager/staff 100-249 employees, $25M-$49.99M revenue, Government

“The serverless database options… are compelling from a cost standpoint.
You go out and pay for what you use and grow as you need. So I haven’t
heard any complaints about the database cost.”
IT/engineering manager/staff
100,000+ employees, $10B+ revenue, Food, beverage & agriculture

## Which of the following pricing models does your organization use to purchase cloud resources today?
## Which role in your organization is primarily responsible for implementing cloud financial management (CFM) practices?
Base: All respondents.
Source: 451 Research custom survey, 2022.
Allocate Costs in a Way That Is Meaningful in Business Terms,
Whether Via Account Management, Resource Tagging or
Chargeback/Showback
“[For public cloud pricing] we use chargeback models. So we set up
accounts in the clouds and then they get charged back… and we take
percentages out for like management and for the different groups that
have to be involved… We’re constantly evaluating how can you reduce
your cost in public cloud; for example, can we move you from Windows
to Linux, or can we put in long-term contracts for one year or even three
years of consumption, you get lower rates.”
IT/engineering manager/staff
100-249 employees, $25M-$49.99M revenue, Government

## Which role in your organization is primarily responsible for
implementing cloud financial management (CFM) practices?
Base: All respondents (n=1,000).
Source: 451 Research custom survey, 2022
Our survey shows that ownership of the
cloud financial management function
rests primarily with IT teams (see Figure 6);
this finding is consistent across spending
levels. In most organizations, IT teams are
responsible for both budgetary discipline
and cloud resource positioning

## How involved is the IT team in setting/meeting your organization’s sustainability goals?
Base: Organizations with a sustainability program in place (n=861)
Source: 451 Research custom survey, 202
IT teams are top
contributors to the effort: 71% of those with sustainability goals cite IT as
being very involved in setting and meeting those goals

## Which of the following software tools do you currently use or plan to use to measure, monitor and manage sustainability
improvements?
Base: Organizations with a sustainability program in place (n=793).
Source: 451 Research custom survey, 2022.
Figure 8: Excel spreadsheets are currently the top tool for tracking
sustainability metrics   

* The Business Value of Adopting Amazon Web Services (AWS) Managed Databases  

- 13% reduction in the costs of licenses/subscriptions associated with databases
- 68% improvement in analytic query execution time
- 51% reduction in unplanned downtime
- 53% reduction in database administration
- 38% increase in developer productivity associated with database-related tasks  

**AWS Cloud Value Framework  

** Cost Savings (TCO)  
50%+ reduction in TCO.   
- GE Oil & Gas is migrating 500 applications to the cloud by the end of 2016 as part of a major digital transformation,   
helping it attain a 52 percent reduction in TCO. GE Oil & Gas is a business unit of global conglomerate General Electric,   
with energy-related operations around the world. The company's cloud migration project entailed reexamining—and in many cases,   
eliminating—legacy processes, resulting not only in lower IT costs but also in greater speed to market and more agility to compete   
even better in an industry experiencing immense market challenges.   

** Staff Productivity  
Over 500 hours per year of server configuration time saved.  
- Sage company    

** Operational Resilience   
Critical workloads run in multiple AZs and Regions for robust DR.   
- Expedia Group is all in on AWS, with plans to migrate 80 percent of its mission-critical apps from its on-premises data    
centers to the cloud in the next two to three years. By using AWS, Expedia Group has become more resilient. 

** Business Agility   
Launch of new products 75% faster.   
- Unilever: Deploying new features/ applications faster and reducing errors   

** Sustainability  
Minimizing environmental impact of business operations   
- Power consumption reduced by 10 megawatts per year.   
-  In fact, according to a 451 Research report, the AWS infrastructure is 3.6 times more energy efficient than the median of the surveyed US enterprise data centers, and AWS operates at an 88% lower carbon footprint.   
- CapitalOne company     

The business value of migrating to AWS to learn how migrating on-premises infrastructure to AWS achieves quantifiable business value such as:  

69% decrease in unplanned downtime  
20% cost savings in technology infrastructure  
29% increase in staff focus on innovation  

**Cloud Adoption Framework (CAF)

The AWS Cloud Adoption Framework (AWS CAF) leverages AWS experience and best practices to help you digitally transform and accelerate your business outcomes through innovative use of AWS.   

![image](https://github.com/souravs17031999/CCP-AWS-CLFC01/assets/33771969/5fae6a04-094b-4c50-881f-c0a90e787b92)        
![test drawio (1)](https://github.com/souravs17031999/CCP-AWS-CLFC01/assets/33771969/2b5d373c-7bd9-4a35-9ed7-b591682b9041)   


## TCO MIGRATION   

Migration Evaluator  
Migration Evaluator (formerly TSO Logic) is a complimentary service to create data-driven business cases for AWS Cloud planning and migration.  

AWS Pricing Calculator   
AWS Pricing Calculator is a web-based service that you can use to create cost estimates to suit your AWS use cases. This service is useful both for people who have never used AWS and for those who want to reorganize or expand their usage.   

![CapEx-OpEx-characteristics-challenges-blog](https://github.com/souravs17031999/CCP-AWS-CLFC01/assets/33771969/13d7cc95-fd0a-4289-8a93-a70bd220e4f5)   

* The Well-Architected Framework identifies a set of general design principles to facilitate good design in the cloud:   

- Stop guessing your capacity needs: With cloud computing, these problems can go away. You can use as much or as little capacity as you need, and scale up and down automatically.   
- Test systems at production scale: In the cloud, you can create a production-scale test environment on demand, complete your testing, and then decommission the resources.    
- Automate with architectural experimentation in mind: Automation permits you to create and replicate your workloads at low cost and avoid the expense of manual effort.   
- Consider evolutionary architectures: This permits systems to evolve over time so that businesses can take advantage of innovations as a standard practice.  
- Drive architectures using data: Your cloud infrastructure is code, so you can use that data to inform your architecture choices and improvements over time.   
- Improve through game days: Test how your architecture and processes perform by regularly scheduling game days to simulate events in production.     

AWS Architecture Center : web portal for Reference architecture examples and diagrams   

## Why moving to cloud computing ?

- Trade upfront expense for variable expense: By taking a cloud computing approach that offers the benefit of variable expense, companies can implement innovative solutions while saving on costs.
- Stop spending money to run and maintain data centers: A benefit of cloud computing is the ability to focus less on these tasks and more on your applications and customers.
- Stop guessing capacity: For example, you can launch Amazon EC2 instances when needed, and pay only for the compute time you use. Instead of paying for unused resources or having to deal with limited capacity, you can access only the capacity that you need. You can also scale in or scale out in response to demand.
- Benefit from massive economies of scale: Because usage from hundreds of thousands of customers can aggregate in the cloud, providers, such as AWS, can achieve higher economies of scale. The economy of scale translates into lower pay-as-you-go prices. 
- Increase speed and agility: This flexibility provides you with more time to experiment and innovate. When computing in data centers, it may take weeks to obtain new resources that you need. By comparison, cloud computing enables you to access new resources within minutes.
- Go global in minutes: The global footprint of the AWS Cloud enables you to deploy applications to customers around the world quickly, while providing them with low latency. This means that even if you are located in a different part of the world than your customers, customers are able to access your applications with minimal delays. 

## The pillars of the framework  (WELL ARCHITECHTED FRAMEWORK)    

![test drawio (2)](https://github.com/souravs17031999/CCP-AWS-CLFC01/assets/33771969/b8c92a9e-da2b-4baf-a367-de0e2b0b04c5)     

### Operational excellence      
* Perform operations as code: You can define your entire workload (applications, infrastructure) as code and update it with code. You can implement your operations procedures as code and automate their run process by initiating them in response to events.
    * Use version control: Maintain assets in version controlled repositories. Doing so supports tracking changes, deploying new versions, detecting changes to existing versions, and reverting to prior versions
    * Fully automate the integration and deployment pipeline from code check-in through build, testing, deployment, and validation. This reduces lead time, encourages increased frequency of change, and reduces the level of effort.  
- Make frequent, small, reversible changes: Design workloads to permit components to be updated regularly. 
    * Plan for unsuccessful changes: Plan to revert to a known good state (that is, roll back the change), or remediate in the production environment 
    * Test changes and validate the results at all lifecycle stages to confirm new features and minimize the risk and impact of failed deployments. 
    * Test with limited deployments alongside existing systems to confirm desired outcomes prior to full scale deployment. For example, use deployment canary testing or one-box deployments.
    * Rolling updates – Elastic Beanstalk applies your configuration changes in batches, keeping a minimum number of instances running and serving traffic at all times. Immutable updates – Elastic Beanstalk launches a temporary Auto Scaling group outside of your environment with a separate set of instances running with the new configuration. Disabled – Elastic Beanstalk makes no attempt to avoid downtime. It terminates your environment's existing instances and replaces them with new instances running with the new configuration.
- Refine operations procedures frequently: Set up regular game days to review and validate that all procedures are effective and that teams are familiar with them.   
    * Use Operational Readiness Reviews (ORRs) to validate that you can operate your workload.
    * ![image](https://github.com/souravs17031999/CCP-AWS-CLFC01/assets/33771969/716f325b-f8b3-417b-8419-8cd62b8e7633)     
    * In cloud operations, we use runbooks to reduce risk and achieve desired outcomes. At its simplest, a runbook is a checklist to complete a task.
- Anticipate failure: Perform “pre-mortem” exercises to identify potential sources of failure so that they can be removed or mitigated.    
- Learn from all operational failures: Drive improvement through lessons learned from all operational events and failures.      
    * Define escalation paths for the teams that support your workload, especially if the team doesn’t have an on-call rotation. Based on your support level, you can also file a case with AWS Support.
    * As a first line of defense, web and mobile applications should provide friendly and informative error messages during an outage as well as have the ability to redirect traffic to a status page.
    * Communicate status through dashboards: Provide dashboards tailored to their target audiences (for example, internal technical teams, leadership, and customers) to communicate the current operating status of the business and provide metrics of interest.
### Security     
- Implement a strong identity foundation: Implement the principle of least privilege and enforce separation of duties with appropriate authorization for each interaction with your AWS resources.   
    * As your teams determine what access is required, remove unneeded permissions and establish review processes to achieve least privilege permissions. 
    * multi-account strategy: Account-level separation is strongly recommended, as it provides a strong isolation boundary for security, billing, and access.
    * Securing the root user helps reduce the chance that accidental or intentional damage can occur through the misuse of root user credentials.
    * Stay up-to-date with both AWS and industry security recommendations to evolve the security posture of your workload. AWS Security Bulletins contain important information about security and privacy notifications.
- Maintain traceability: Monitor, alert, and audit actions and changes to your environment in real time. Integrate log and metric collection with systems to automatically investigate and take action.    
- Apply security at all layers: Apply a defense in depth approach with multiple security controls   
   * Don't put all resources in single VPC. Use multi-layered approach and check which resources needs internet access and which not.
   * Control traffic at all layers: VPC's, At edge etc...
   * Harden operating system: Configure operating systems to meet best practices. Harden security in all code to reduce attack surface.
   * Investigate mechanisms: Code signing is one mechanism that can be used to validate software integrity.
- Automate security best practices: Automated software-based security mechanisms improve your ability to securely scale more rapidly and cost-effectively.    
   * Perform threat modeling to identify and maintain an up-to-date register of potential threats and associated mitigations for your workload.
   *  Shostack’s 4 Question Frame for Threat Modeling: What are we working on ? What can go wrong ? What can we do about it ? Did we do a great Job ?
- Protect data in transit and at rest: Classify your data into sensitivity levels and use mechanisms, such as encryption, tokenization, and access control where appropriate.   
   * Data classification allows workload owners to identify locations that store sensitive data and determine how that data should be accessed and shared.
   * Encryption maintains the confidentiality of sensitive data in the event of unauthorized access or accidental disclosure.
   * All data should be encrypted in transit using secure TLS protocols and cipher suites. Network traffic between your resources and the internet must be encrypted to mitigate unauthorized access to the data.
- Keep people away from data: Use mechanisms and tools to reduce or eliminate the need for direct access or manual processing of data. This reduces the risk of mishandling or modification and human error when handling sensitive data.    
   * Avoiding the use of long-term credentials in favor of temporary credentials should go hand in hand with a strategy of reducing the usage of IAM users in favor of federation and IAM roles.
   * Implement a user access lifecycle policy for new users joining, job function changes, and users leaving so that only current users have access.
- Prepare for security events: Prepare for an incident by having incident management and investigation policy and processes that align to your organizational requirements.     
   * A process that allows emergency access to your workload in the unlikely event of an automated process or pipeline issue. This will help you rely on least privilege access, but ensure users can obtain the right level of access when they require it. 
   * An organization should be able to reliably and consistently retrieve security event logs from AWS services and applications in a timely manner when required to fulfill an internal process or obligation, such as a security incident response.
### Reliability       
- Automatically recover from failure: By monitoring a workload for key performance indicators (KPIs), you can start automation when a threshold is breached.     
   * Be aware of your default quotas and manage your quota increase requests for your workload architecture. Know which cloud resource constraints, such as disk or network, are potentially impactful.
   * Check for if the service has any hard limits using Stress testing, load testing, and chaos testing.
   * Verify that your quotas cover the overlap of failed or inaccessible resources and their replacements. You should consider use cases like network failure, Availability Zone failure, or Regional failures when calculating this gap.
   * Prefer hub-and-spoke topologies over many-to-many mesh. If more than two network address spaces (VPCs, on-premises networks) are connected via VPC peering, AWS Direct Connect, or VPN, then use a hub-and-spoke model like that provided by AWS Transit Gateway.
   * An idempotent service makes it easier for a client to implement retries without fear that a request will be erroneously processed multiple times.    

![image](https://github.com/souravs17031999/CCP-AWS-CLFC01/assets/33771969/edd2569c-e739-4201-bb00-ce0c6ef816b4)    

   * Implement graceful degradation to transform applicable hard dependencies into soft dependencies. When a component's dependencies are unhealthy, the component itself can still function, although in a degraded manner. For example, when a dependency call fails, failover to a predetermined static response. Ex. circuit breaker pattern etc...

![image](https://github.com/souravs17031999/CCP-AWS-CLFC01/assets/33771969/db72128a-c20d-4c1f-ae56-7f600aac03ab)  
   * Throttle requests. This is a mitigation pattern to respond to an unexpected increase in demand. Some requests are honored but those over a defined limit are rejected and return a message indicating they have been throttled.
   * Control and limit retry calls. Use exponential backoff to retry after progressively longer intervals. Introduce jitter to randomize those retry intervals, and limit the maximum number of retries.
   * If the workload is unable to respond successfully to a request, then fail fast. This allows the releasing of resources associated with a request, and permits the service to recover if it’s running out of resources. Limit queues In a queue-based system, when processing stops but messages keep arriving, the message debt can accumulate into a large backlog, driving up processing time.
   * Set both a connection timeout and a request timeout on any remote call, and generally on any call across processes.

- Test recovery procedures: In an on-premises environment, testing is often conducted to prove that the workload works in a particular scenario. Testing is not typically used to validate recovery strategies.    
   * Validate that your backup process implementation meets your Recovery Time Objectives (RTO) and Recovery Point Objectives (RPO) by performing a recovery test.
   * ![image](https://github.com/souravs17031999/CCP-AWS-CLFC01/assets/33771969/b26e0c77-0eb7-4361-a207-05647eea1348)  
   * ![image](https://github.com/souravs17031999/CCP-AWS-CLFC01/assets/33771969/f390cab7-2fb9-4de7-a098-2ba91f1e1d8a)   

- Scale horizontally to increase aggregate workload availability: Replace one large resource with multiple small resources to reduce the impact of a single failure on the overall workload.  
   *  Services should either not require state, or should offload state such that between different client requests, there is no dependence on locally stored data on disk and in memory. This allows servers to be replaced at will without causing an availability impact.
   ![image](https://github.com/souravs17031999/CCP-AWS-CLFC01/assets/33771969/2adc60e8-32bc-4f16-b44a-f5322a9a7621)  
   
   * After examining whether the state is required, move any state tracking to a resilient multi-zone cache or data store like Amazon ElastiCache, Amazon RDS, Amazon DynamoDB, or a third-party distributed data solution.   
- Stop guessing capacity: A common cause of failure in on-premises workloads is resource saturation, when the demands placed on a workload exceed the capacity of that workload    
   * AWS Auto Scaling lets you detect and replace impaired instances. It also lets you build scaling plans for resources
   * Amazon EC2 Auto Scaling helps you ensure that you have the correct number of Amazon EC2 instances available to handle the load for your application.
   * Amazon DynamoDB auto scaling uses the AWS Application Auto Scaling service to dynamically adjust provisioned throughput capacity on your behalf, in response to actual traffic patterns.
- Manage change in automation: Changes to your infrastructure should be made using automation.
   * Organizations that need to know, receive notifications when significant events occur. Alerts can be sent to Amazon Simple Notification Service (Amazon SNS) topics, and then pushed to any number of subscribers. For example, Amazon SNS can forward alerts to an email alias so that technical staff can respond.
   * Integrate functional testing as part of your deployment. Functional tests are run as part of automated deployment. If success criteria are not met, the pipeline is halted or rolled back.
   * ![image](https://github.com/souravs17031999/CCP-AWS-CLFC01/assets/33771969/702f72e9-5e66-47cb-a93a-75131e3795be)    

single-AZ resiliency:     
   * 2 9s (99%) scenario, 3 9s (99.9%) scenario, 4 9s (99.99%) scenario   
Multi-AZ resiliency:   
   * 3½ 9s (99.95%) with a Recovery Time between 5 and 30 Minutes, 5 9s (99.999%) or higher scenario with a recovery time under one minute
     
### Performance efficiency      
- Democratize advanced technologies: Make advanced technology implementation smoother for your team by delegating complex tasks to your cloud vendor.    
   * Understand the process of techology selection : Use existing workloads for load testing, use reference aws architechtures or help from AWS partner networks or aws solution architects etc...
- Go global in minutes: Deploying your workload in multiple AWS Regions around the world permits you to provide lower latency and a better experience for your customers at minimal cost.   
- Use serverless architectures: Serverless architectures remove the need for you to run and maintain physical servers for traditional compute activities.   
- Experiment more often: With virtual and automatable resources, you can quickly carry out comparative testing using different types of instances, storage, or configurations.   
   * To optimize performance, overall efficiency, and cost effectiveness, determine first which resources your workload needs. Choose memory-optimized instances, such as the R-family of instances, for memory-intensive workloads like a database. For workloads that require higher compute capacity, choose the C-family of instances, or choose instances with higher core counts or higher core frequency. Choose I/O performance based on the needs of your workload instead of comparing against standard, synthetic benchmarks. For higher I/O performance, choose instances from the I-family of instances.
   * Collect compute-related metrics over time. Compare workload metrics against available resources.
- Consider mechanical sympathy: Understand how cloud services are consumed and always use the technology approach that aligns with your workload goals.  
   * Use a data-driven approach to evolve your architecture: As you make changes to the workload, collect and evaluate metrics to determine the impact of those changes. Measure the impacts to the system and to the end-user to understand how your tradeoffs impact your workload. Use a systematic approach, such as load testing, to explore whether the tradeoff improves performance.   
 
### Cost optimization     
- Implement Cloud Financial Management: To achieve financial success and accelerate business value realization in the cloud, invest in Cloud Financial Management and Cost Optimization.   
   * Establish a Cloud Business Office (CBO) or Cloud Center of Excellence (CCOE) team that is responsible for establishing and maintaining a culture of cost awareness in cloud computing.
   * Report cloud costs to technology teams: To raise cost awareness, and establish efficiency KPIs for finance and business stakeholders.

![image](https://github.com/souravs17031999/CCP-AWS-CLFC01/assets/33771969/76aeed90-575c-429e-a54a-428526325301)   

- Adopt a consumption model: Pay only for the computing resources that you require and increase or decrease usage depending on business requirements, not by using elaborate forecasting.   
   * AWS has multiple pricing models that allow you to pay for your resources in the most cost-effective way that suits your organization’s needs and depending on product. Work with your teams to determine the most appropriate pricing model.   
- Measure overall efficiency: Measure the business output of the workload and the costs associated with delivering it.   
- Stop spending money on undifferentiated heavy lifting: AWS does the heavy lifting of data center operations like racking, stacking, and powering servers.   
- Analyze and attribute expenditure: The cloud makes it simple to accurately identify the usage and cost of systems, which then permits transparent attribution of IT costs to individual workload owners.      
![image](https://github.com/souravs17031999/CCP-AWS-CLFC01/assets/33771969/63a94a97-1d29-4698-8a63-cc386df1bc18)   
   * Decommission workload resources that are no longer required. A common example is resources used for testing: after testing has been completed, the resources can be removed. Example values for feature tagging are feature-X testing to identify the purpose of the resource in terms of the workload lifecycle. Another example is using LifeSpan or TTL for the resources, such as to-be-deleted tag key name and value to define the time period or specific time for decommissioning.
![image](https://github.com/souravs17031999/CCP-AWS-CLFC01/assets/33771969/8645232d-7488-4d2e-b08d-5a912e83c360)  



### Sustainability       
- Understand your impact: Measure the impact of your cloud workload and model the future impact of your workload.   

![image](https://github.com/souravs17031999/CCP-AWS-CLFC01/assets/33771969/86b81535-d386-4942-b154-b8a09a3d1dfa)   

- Establish sustainability goals: For each cloud workload, establish long-term sustainability goals such as reducing the compute and storage resources required per transaction.   
- Maximize utilization: Right-size workloads and implement efficient design to verify high utilization and maximize the energy efficiency of the underlying hardware.    
   * Adding sustainability to your list of business requirements can result in more cost-effective solutions. Focusing on getting more value from the resources you use and using fewer of them directly translates to cost savings on AWS as you pay only for what you use.
- Anticipate and adopt new, more efficient hardware and software offerings: Support the upstream improvements your partners and suppliers make to help you reduce the impact of your cloud workloads.   
- Use managed services: Sharing services across a broad customer base helps maximize resource utilization, which reduces the amount of infrastructure needed to support cloud workloads.    
- Reduce the downstream impact of your cloud workloads: Reduce the amount of energy or resources required to use your services.      
- Best practices:
   * The AWS Cloud is a constantly expanding network of Regions and points of presence (PoP), with a global network infrastructure linking them together. The choice of Region for your workload significantly affects its KPIs, including performance, cost, and carbon footprint.
   * Use elasticity of the cloud and scale your infrastructure dynamically to match supply of cloud resources to demand and avoid overprovisioned capacity in your workload.
   * Review and optimize workload service-level agreements (SLA) based on your sustainability goals to minimize the resources required to support your workload while continuing to meet business needs.
   * Select cloud location and services for your workload that reduce the distance network traffic must travel and decrease the total network resources required to support your workload.
   * Leverage serverless on AWS to eliminate over-provisioned infrastructure.
   * Right size individual components of your architecture to prevent idling resources waiting for input.
   * Classify data to understand its criticality to business outcomes and choose the right energy-efficient storage tier to store the data.
   * Use the criticality of your data classification and design backup strategy based on your recovery time objective (RTO) and recovery point objective (RPO). Avoid backing up non-critical data.
   * Using Managed device farms can help you to streamline the testing process for new features on a representative set of hardware. Managed device farms offer diverse device types including older, less popular hardware, and avoid customer sustainability impact from unnecessary device upgrades.   

### SIX R's of Migration

![test drawio (3)](https://github.com/souravs17031999/CCP-AWS-CLFC01/assets/33771969/0a88c7e9-eb15-4e24-a245-c91fdae5fdc1)   


# SECURITY AND COMPLIANCE 

**Applying the AWS Shared Responsibility Model in Practice    

Customer responsibility varies based on many factors, including the AWS services and Regions they choose, the integration of those services into their IT environment, and the laws and regulations applicable to their organization and workload.

**AWS Cloud Adoption Framework (AWS CAF)
- AWS Cloud Adoption Framework (AWS CAF) leverages AWS experience and best practices to help you digitally transform and accelerate your business outcomes through innovative use of AWS. AWS CAF groups its capabilities in six perspectives: Business, People, Governance, Platform, Security, and Operations.
- Review the security functionality and configuration options of individual AWS services within the security chapters of AWS service documentation.
- Determine external and internal security and related compliance requirements and objectives, and consider industry frameworks like the NIST Cybersecurity Framework (CSF) and ISO.
- The National Institute of Standards and Technology (NIST) 800-53 security controls are generally applicable to US Federal Information Systems.  AWS has received FedRAMP Authorizations to Operate (ATO) from multiple authorizing agencies for both AWS GovCloud (US) and the AWS US East/West Region.
- Provide your internal and external audit teams with cloud-specific learning opportunities by leveraging the Cloud Audit Academy training programs.
- Perform a Well-Architected Review of your AWS workloads to evaluate the implementation of best practices for security, reliability, and performance.
- Explore solutions available in the AWS Marketplace digital catalog with thousands of software listings from independent software vendors that enable you to find, test, buy, and deploy software that runs on AWS.
- Explore AWS Security Competency Partners offering expertise and proven customer success securing every stage of cloud adoption, from initial migration through ongoing day-to-day management

![image](https://github.com/souravs17031999/CCP-AWS-CLFC01/assets/33771969/ee81511b-2ef9-4960-b8e6-28bd0f06750b)

Shared responsibility for NIST (National Institute of Standards and Technology)
Shared Responsibility: You will provide security and configurations of your software components and AWS will provide security for its infrastructure.
Customer-Only Responsibility: You are fully responsible for guest operating systems, deployed applications, and select networking resources (for example, firewalls). More specifically, you are solely responsible for configuring and managing your security in the cloud.
AWS-Only Responsibility: AWS manages the cloud infrastructure, including the network, data storage, system resources, data centers, physical security, reliability, and supporting hardware and software. Applications built on top of the AWS system inherit the features and configurable options that AWS provides. AWS is solely responsible for configuring and managing security of the cloud.

AWS has certification for compliance with ISO/IEC 27001:2013, 27017:2015, 27018:2019, 27701:2019, 22301:2019, 9001:2015, and CSA STAR CCM v4.0.

**Shared responsbility model and how it changes based on services : 

![image](https://github.com/souravs17031999/CCP-AWS-CLFC01/assets/33771969/9b9f9623-2107-444c-a698-4bc0a8273f86)  
  
![image](https://github.com/souravs17031999/CCP-AWS-CLFC01/assets/33771969/e8c3322a-eb23-4986-9fcb-3114acf1d561)

=============================================

### AWS Artifact
- Access AWS and ISV security and compliance reports
Use AWS Artifact reports to access several compliance reports from third-party auditors who have tested and verified our compliance with a variety of global, regional, and industry specific security standards and regulations.

**AWS Compliance Programs
- The AWS Compliance Program helps customers to understand the robust controls in place at AWS to maintain security and compliance of the cloud.
- Place to find all compliance related information 

**HIPPA (The Health Insurance Portability and Accountability Act)
- is legislation that is designed to make it easier for US workers to retain health insurance coverage when they change or lose their jobs. The legislation also seeks to encourage electronic health records to improve the efficiency and quality of the US healthcare system through improved information sharing.
- Along with increasing the use of electronic medical records, HIPAA includes provisions to protect the security and privacy of protected health information (PHI). PHI includes a very wide set of personally identifiable health and health-related data, including insurance and billing information, diagnosis data, clinical care data, and lab results such as images and test results.
- There is no HIPAA certification for a cloud service provider (CSP) such as AWS. In order to meet the HIPAA requirements applicable to our operating model, AWS aligns our HIPAA risk management program with FedRAMP and NIST 800-53, which are higher security standards that map to the HIPAA Security Rule.

**AWS SOC (AWS System and Organization Controls)
- Reports are independent third-party examination reports that demonstrate how AWS achieves key compliance controls and objectives. The purpose of these reports is to help you and your auditors understand the AWS controls established to support operations and compliance.

 At a high level, describe how customers achieve compliance on AWS

- AWS Audit Manager : Continuously audit your AWS usage to simplify how you assess risk and compliance with regulations and industry standards
- Amazon GuardDuty: Protect your AWS accounts and workloads with intelligent threat detection and continuous monitoring
- Amazon Artifact: No cost, self-service portal for on-demand access to AWS’ compliance reports
- Amazon Data Centers: Learn about our security approach to protect the data of millions of active monthly customers

AWS gives you control that can help you comply with the regional and
local data privacy laws and regulations applicable to your organization.
The design of our global infrastructure allows you to retain complete
control over the locations in which your data is physically stored,
which can help you meet data residency requirements. 

Reduce risk and enable growth by using our activity monitoring
services that detect configuration changes and security events across
your system, even integrating our services with your existing solutions
to help simplify your operations and compliance reporting. 

Ensure that your resources have the right level of access at all times by
leveraging fine-grain identity and access controls and continuous
monitoring for near real-time security information – regardless of
where your information in stored. 

You retain complete control over which AWS Region(s) your data is physically stored in, which can
help you meet your compliance and data residency requirements. For example, if you are a
European customer, you can choose to deploy your AWS services exclusively in the EU (Frankfurt)
Region. If you make this choice, your content will be exclusively stored in Germany unless you
select a different AWS Region. 

===================================================
### Data Encryption

- Data at rest encryption capabilities available in most AWS services, such as Amazon EBS, Amazon S3, Amazon RDS, Amazon Redshift, Amazon ElastiCache, AWS Lambda, and Amazon SageMaker
- Flexible key management options, including AWS Key Management Service, that allow you to choose whether to have AWS manage the encryption keys or enable you to keep complete control over your own keys
- Dedicated, hardware-based cryptographic key storage using AWS CloudHSM, allowing you to help satisfy your compliance requirements
- Encrypted message queues for the transmission of sensitive data using server-side encryption (SSE) for Amazon SQS
- In addition, AWS provides APIs for you to integrate encryption and data protection with any of the services you develop or deploy in an AWS environment.

ategory	What is it	AWS service
Identity and access management	Securely manage identities and access to AWS services and resources  	AWS Identity and Access Management (IAM)
- Apply fine-grained permissions and scale with attribute-based access control, Manage per-account access or scale access across AWS accounts and applications, 

Centrally manage workforce access to multiple AWS accounts and applications	AWS IAM Identity Center (successor to SSO)
- Enable multi-account access to your AWS accounts, Enable single sign-on access to your AWS applications 

AWS ACCOUNTS AND ORGANIZATIONS BEST PRACTICES : 
![image](https://github.com/souravs17031999/CCP-AWS-CLFC01/assets/33771969/de330689-ef8c-4fd2-bdfc-1d08cf869586)

### AWS IAM VS IDENTIY CENTER 

AWS Identity and Access Management enables admins to manage access to AWS services and resources within an AWS account securely for what it calls “entities” — IAM users created from the AWS IAM admin console, federated users, application code, or another AWS service.

AWS IAM Identity Center manages access for all AWS accounts within an AWS Organization, as well as access to other cloud applications, e.g., Salesforce. AWS IAM Identity Center includes a user portal where end users can find and access their assigned AWS accounts, cloud applications, and custom applications in one place.

![image](https://github.com/souravs17031999/CCP-AWS-CLFC01/assets/33771969/a50fc633-1154-4a95-b1f6-7c25dc5d38e7)


Implement secure, frictionless customer identity and access management (CIAM) that scales	Amazon Cognito
- Engage customers: Allow customers to sign in directly, or through social or enterprise identity providers, to a hosted UI with your branding.
- Manage B2B identities: Use a variety of multi-tenancy options that provide different levels of policy and tenant isolation for your business.
- Secure machine-to-machine authentication: Develop modern, secure, microservice-based applications, and more easily connect your application to backend resources and web services.
- Get role-based access to AWS resources: Gain secure, role-based access to AWS services, such as Amazon S3, Amazon DynamoDB, and AWS Lambda.

Manage fine-grained permissions and authorization within custom applications	Amazon Verified Permissions (preview)
- gave developers provide the way to define their own permissions model using **cedar** policy

Gain efficiency with a fully managed Microsoft Active Directory service	AWS Directory Service
- Integrate your on-premises credentials

Simply and securely share your AWS resources across multiple accounts	AWS Resource Access Manager (AWS RAM)
- Share resources in multi-account environments

Centrally manage your environment as you scale your AWS resources	AWS Organizations
- Automate AWS account creation
- Enable proactive protection with a dedicated security group

### Detection
**Automate AWS security checks and centralize security alerts	AWS Security Hub
- Conduct Cloud Security Posture Management (CSPM), Security scanning, Triage and prioritize security issues, Correlate your security findings to discover new insights, Initiate Security Orchestration, Automation, and Response (SOAR) workflows

**Protect AWS accounts with intelligent threat detection	Amazon GuardDuty
- Improve security operations visibility: unknonwn IP malicious, suspicious logins, unusual data access;
- Identify files containing malware: scan EBS 
- Identify and profile possible malicious or suspicious behavior in container workloads by analyzing Amazon EKS audit logs and container runtime activity.

**Automated and continual vulnerability management at scale	Amazon Inspector
- Quickly discover vulnerabilities, Use up-to-date common vulnerabilities and exposures (CVE), Support compliance requirements and best practices for NIST CSF, PCI DSS; Identify zero-day vulnerabilities sooner
- Mean Time to Respond (MTTR) : Accelerate MTTR

**Automatically centralize your security data in a few steps 	Amazon Security Lake (Preview)
- Analyze multiple years of security data quickly, Support on-demand analysis of petabyte-scale data

**Assess, audit, and evaluate configurations of your resources	AWS Config
- Discover resources that exist in your account or publish the configuration data of third-party resources into AWS Config, record their configurations, and capture any changes
- Codify your compliance requirements as AWS Config rules and author remediation actions, automating the assessment of your resource configurations across your organization. (compliance as a code)
- Continually audit security monitoring and analysis  

![image](https://github.com/souravs17031999/CCP-AWS-CLFC01/assets/33771969/294f56e2-6985-4be4-a055-bbd4e651c0e3)  

**Observe and monitor resources and applications on AWS, on premises, and on other clouds	Amazon CloudWatch
- Visualize performance data, create alarms, and correlate data to understand and resolve the root cause of performance issues in your AWS resources.
- Analyze metrics, logs, logs analytics, and user requests to speed up debugging and reduce overall mean time to resolution.
- Find out exactly when your website is impacted and for how long by viewing screenshots, logs, and web requests at any point in time.

![image](https://github.com/souravs17031999/CCP-AWS-CLFC01/assets/33771969/c64bda19-d37f-4308-9248-7cd6e2cbd793)


**Track user activity and API usage	AWS CloudTrail
- Audit activity: Monitor, store, and validate activity events for authenticity.
- Identify security incidents: Detect unauthorized access using the Who, What, and When information in CloudTrail Events.
- Troubleshoot operational issues: Continuously monitor API usage history using machine learning (ML) models to spot unusual activity in your AWS accounts, and determine root cause.
- prove compliance with regulations such as SOC, PCI, and HIPAA.
- Improve your security posture by recording user activity and events, and set up automated workflow rules with Amazon EventBridge.

**Security management across your IoT devices and fleets	AWS IoT Device Defender
- Audit the security posture of IoT resources across your device fleet to easily identify gaps and vulnerabilities.
- Detect the use of insecure network services and protocols with known security weaknesses,

**Network and application protection	Centrally configure and manage firewall rules across your accounts	AWS Firewall Manager
- Deploy managed rules, such as pre-configured WAF rules on your applications, across accounts.

**Deploy network firewall security across your VPCs	AWS Network Firewall
- Inspect inbound internet traffic
- Filter outbound traffic
- Secure Direct Connect and VPN traffic from client devices and your on-premises environments supported by AWS Transit Gateway.

**Maximize application availability and responsiveness with managed DDoS protection	AWS Shield
- Protect applications and APIs from SYN floods, UDP floods, or other reflection attacks.
- AWS automatically mitigates network and transport layer (layer 3 and layer 4) Distributed Denial of Service (DDoS) attacks.
- For application layer (layer 7) DDoS attacks, AWS attempts to detect and notify AWS Shield Advanced customers through CloudWatch alarms. By default, it doesn't automatically apply mitigations, to avoid inadvertently blocking valid user traffic.
- The SRT (Shield Response Team (SRT)) helps you triage the DDoS attack to identify attack signatures and patterns. With your consent, the SRT creates and deploys AWS WAF rules to mitigate the attack.

**Provide secure access to corporate applications without a VPN	AWS Verified Access (Preview)
- verified each request with access token in real time as per rules defined 

**Protect your web applications from common exploits	AWS Web Application Firewall (WAF)
- you can create security rules that control bot traffic and block common attack patterns such as SQL injection or cross-site scripting (XSS).
- Create rules to filter web requests based on conditions such as IP addresses, HTTP headers and body, or custom URIs.

Filter and control outbound DNS traffic for your VPCs	Amazon Route 53 Resolver DNS Firewall

**Data protection	Discover and protect your sensitive data at scale	_Amazon Macie_
- machine learning (ML) and pattern matching to discover and help protect your sensitive data.

**Create and control keys to encrypt or digitally sign your data	AWS Key Management Service (AWS KMS)
- Encrypt data within your applications with the AWS Encryption SDK data encryption library.
- Perform signing operations using asymmetric key pairs to validate digital signatures.
- Securely generate hash-based message authentication codes (HMACs) that ensure message integrity and authenticity.
- Protect your data at rest

**Manage single-tenant hardware security modules (HSMs) on AWS	AWS CloudHSM


**Provision and manage SSL/TLS certificates with AWS services and connected resources	AWS Certificate Manager
- to provision, manage, and deploy public and private SSL/TLS certificates for use with AWS services and your internal connected resources. ACM removes the time-consuming manual process of purchasing, uploading, and renewing SSL/TLS certificates.
- 

**Create private certificates to identify resources and protect data	AWS Private Certificate Authority 
- AWS Private CA) is a highly available, versatile CA that helps organizations secure their applications and devices using private certificates.

**Centrally manage the lifecycle of secrets	AWS Secrets Manager
- Securely encrypt and centrally audit secrets such as database credentials and API keys.
- Rotate secrets automatically to meet your security and compliance requirements.
- Centrally store and manage credentials, API keys, and other secrets.

**Incident response	Analyze and visualize security data to investigate potential security issues	Amazon Detective
- Analyze and visualize security data to investigate potential security issues
- Determine the extent of malicious activity, its impact, and the underlying cause by analyzing relevant historical activities for patterns.

**Scalable, cost-effective application recovery to AWS	AWS Elastic Disaster Recovery
- Quickly recover operations after unexpected events such as software issues or datacenter hardware failures. AWS DRS enables RPOs of seconds and RTOs of minutes.

**Compliance	No cost, self-service portal for on-demand access to AWS’ compliance reports	AWS Artifact
- Find auditor-issued reports, certifications, accreditations, and other third-party attestations of AWS in a comprehensive resource.
- Review, accept, and manage your agreements with AWS
- Perform due-diligence of ISVs that sell products on AWS Marketplace, with on-demand access to their security and compliance reports.

**Continually audit your AWS usage to simplify risk and compliance assessment	AWS Audit Manager
- Automatically collect evidence, monitor your compliance posture, and proactively reduce risk by fine-tuning your controls.
- Avoid the need to collect, review, and manage evidence with automated evidence collection.

==============================================================================

### AWS IAM IN DETAIL

When you create an AWS account, you begin with one sign-in identity that has complete access to all AWS services and resources in the account. This identity is called the AWS account root user and is accessed by signing in with the email address and password that you used to create the account. 

- Set and manage guardrails and fine-grained access controls for your workforce and workloads.
- Manage identities across single AWS accounts or centrally connect identities to multiple AWS accounts.
- Grant temporary security credentials for workloads that access your AWS resources.
- Continually analyze access to right-size permissions on the journey to least privilege.


**Attribute-Based Access Control (ABAC)** : authorization strategy that lets you create fine-grained permissions based on user attributes, such as department, job role, and team name. With ABAC, multiple users using the same IAM role can still get unique, fine-grained access because permissions are based on user attributes. 

A tag is a custom attribute label that you can assign to an AWS resource. Each tag has two parts:
A tag key (for example, CostCenter, Environment, Project, or Purpose). Tag keys are case sensitive.
An optional field known as a tag value (for example, 111122223333, Production, or a team name). Omitting the tag value is the same as using an empty string. Like tag keys, tag values are case sensitive.


**Temporary security credentials: You can use the AWS Security Token Service (AWS STS) to create and provide trusted users with temporary security credentials that can control access to your AWS resources.

- Access keys are long-term credentials for an IAM user or the AWS account root user. You can use access keys to sign programmatic requests to the AWS CLI or AWS API
Access keys consist of two parts: an access key ID (for example, AKIAIOSFODNN7EXAMPLE) and a secret access key (for example, wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY). You must use both the access key ID and secret access key together to authenticate your requests.

- You can set a custom password policy on your AWS account to specify complexity requirements and mandatory rotation periods for your IAM users' passwords. 
If you don't set a custom password policy, IAM user passwords must meet the default AWS password policy. The IAM password policy does not apply to the AWS account root user password or IAM user access keys. 
If a password expires, the IAM user can't sign in to the AWS Management Console but can continue to use their access keys. 

The default password policy enforces the following conditions:

Minimum password length of 8 characters and a maximum length of 128 characters

Minimum of three of the following mix of character types: uppercase, lowercase, numbers, and non-alphanumeric character (! @ # $ % ^ & * ( ) _ + - = [ ] { } | ')
Not be identical to your AWS account name or email address
Never expire password

You can't create a "lockout policy" to lock a user out of the account after a specified number of failed sign-in attempts.


**MFA 

for scenarios in which you need an IAM user or root user in your account, require MFA for additional security. With MFA, users have a device that generates a response to an authentication challenge. Each user's credentials and device-generated response are required to complete the sign-in process.

You can enable MFA for the AWS account root user and IAM users. When you enable MFA for the root user, it affects only the root user credentials. 
IAM users in the account are distinct identities with their own credentials, and each identity has its own MFA configuration. 
You can register up to eight MFA devices of any combination of the currently supported MFA types with your AWS account root user and IAM users. 

How to acheive it ?

- FIDO security key – FIDO Certified hardware security keys are provided by third-party providers.: Public key cryptography
- Virtual MFA devices – A virtual authenticator application that runs on a phone or other device and emulates a physical device. Virtual authenticator apps implement the time-based one-time password (TOTP) algorithm and support multiple tokens on a single device. The user must type a valid code from the device on a second webpage during sign-in.
- A hardware device that generates a six-digit numeric code based on the time-based one-time password (TOTP) algorithm. The user must type a valid code from the device on a second webpage during sign-in.


**IAM terms : 

An AWS Identity and Access Management (IAM) user is an entity that you create in AWS. The IAM user represents the human user or workload who uses the IAM user to interact with AWS.
An IAM user with administrator permissions is not the same thing as the AWS account root user.

An Amazon Resource Name (ARN) for the IAM user. You use the ARN when you need to uniquely identify the IAM user across all of AWS.

Each IAM user is associated with one and only one AWS account.'

An IAM role is an IAM identity that you can create in your account that has specific permissions. An IAM role is similar to an IAM user, in that it is an AWS identity with permission policies that determine what the identity can and cannot do in AWS. However, instead of being uniquely associated with one person, a role is intended to be assumable by anyone who needs it.


**Roles can be used by the following:

An IAM user in the same AWS account as the role
An IAM user in a different AWS account than the role
A web service offered by AWS such as Amazon Elastic Compute Cloud (Amazon EC2)
An external user authenticated by an external identity provider (IdP) service that is compatible with SAML 2.0 or OpenID Connect, or a custom-built identity broker.

AWS service role: A service role is an IAM role that a service assumes to perform actions on your behalf. An IAM administrator can create, modify, and delete a service role from within IAM
AWS service role for an EC2 instance: This role is assigned to the EC2 instance when it is launched

A service-linked role is a unique type of IAM role that is linked directly to an AWS service. Service-linked roles are predefined by the service and include all the permissions that the service requires to call other AWS services on your behalf.

You manage access in AWS by creating policies and attaching them to IAM identities or AWS resources.
During authorization, the AWS enforcement code uses values from the request context to check for matching policies and determine whether to allow or deny the request.
AWS checks each policy that applies to the context of the request. If a single policy denies the request, AWS denies the entire request and stops evaluating policies. This is called an explicit deny.

- An AWS managed policy is a standalone policy that is created and administered by AWS. Standalone policy means that the policy has its own Amazon Resource Name (ARN) that includes the policy name. 
AWS managed policies make it convenient for you to assign appropriate permissions to users, groups, and roles. It is faster than writing the policies yourself, and includes permissions for many common use cases. 
You cannot change the permissions defined in AWS managed policies. AWS occasionally updates the permissions defined in an AWS managed policy.

- You can create standalone policies in your own AWS account that you can attach to principal entities (users, groups, and roles). You create these customer managed policies for your specific use cases, and you can change and update them as often as you like.

- An inline policy is a policy created for a single IAM identity (a user, group, or role). Inline policies maintain a strict one-to-one relationship between a policy and an identity. They are deleted when you delete the identity. 

![image](https://github.com/souravs17031999/CCP-AWS-CLFC01/assets/33771969/3a5c2692-522f-43c2-b215-6dcbb2007d9a)
The first Resource element specifies arn:aws:s3:::test for the ListBucket action so that applications can list all objects in the test bucket. The second Resource element specifies arn:aws:s3:::test/* for the GetObject, PutObject, and DeletObject actions so that applications can read, write, and delete any objects in the test bucket.

![image](https://github.com/souravs17031999/CCP-AWS-CLFC01/assets/33771969/b49044d6-c17a-4671-9019-bd042c34add4)
With the additional statement, users can view the test bucket by using the console. Without those permissions, access is denied. The console lists all buckets in the account, but users cannot view the contents of any other bucket. The read-write permissions are specified only for the test bucket, just like in the previous policy. If a user tries to view another bucket, access is denied.

**Tasks that require root user credentials

- Change your account settings: This includes the account name, email address, root user password, and root user access keys
- Restore IAM user permissions.
- Activate IAM access to the Billing and Cost Management console.
- View certain tax invoices. 
- Close your AWS account.
- Register as a seller in the Reserved Instance Marketplace.
- Configure an Amazon S3 bucket to enable MFA (multi-factor authentication).
- Edit or delete an Amazon Simple Storage Service (Amazon S3) bucket policy 
- Sign up for AWS GovCloud (US). Request AWS GovCloud (US) account root user access keys from AWS Support.

One of the best ways to protect your account is to not have access keys for your AWS account root user.

**IAM policy structure 

![image](https://github.com/souravs17031999/CCP-AWS-CLFC01/assets/33771969/31a88f25-2fe7-40d3-9599-56e881ac4280)  

==============================================================
**AWS TRUSTED ADVISOR 

- Cost optimization:  Examples include identifying idle RDS DB instances, underutilized EBS volumes, unassociated Elastic IP addresses, and excessive timeouts in Lambda functions.
- Performance: Examples include analyzing EBS throughput and latency, compute usage of EC2 instances, and configurations on CloudFront.
- Security: Examples include identifying RDS security group access risk, exposed access keys, and unnecessary S3 bucket permissions.
- Fault tolerant: Examples include examining Auto scaling EC2 groups, deleted health checks on Route 53, disabled Availability Zones, and disabled RDS backups.
- Service Quotas: Trusted Advisor will notify you once you reach more than 80% of a service quota. 

A security group controls the traffic that is allowed to reach and leave the resources that it is associated with. For example, after you associate a security group with an EC2 instance, it controls the inbound and outbound traffic for the instance. You can associate a security group only with resources in the VPC for which it is created.
You can add rules to the security group for the load balancer to allow HTTP and HTTPS traffic from the internet. You can add rules to the security group for the web servers to allow traffic only from the load balancer. You can add rules to the security group for the database servers to allow only database requests from the web servers.

A network access control list (ACL) allows or denies specific inbound or outbound traffic at the subnet level.
Your VPC automatically comes with a modifiable default network ACL. By default, it allows all inbound and outbound IPv4 traffic and, if applicable, IPv6 traffic.
A network ACL has inbound rules and outbound rules. Each rule can either allow or deny traffic.
We evaluate the rules in order, starting with the lowest numbered rule, when deciding whether allow or deny traffic. If the traffic matches a rule, the rule is applied and we do not evaluate any additional rules.

The following are the parts of a network ACL rule: rule number, type, port, port range, source, destination, allow/deny 

**Compare security groups and network ACLs
The following table summarizes the basic differences between security groups and network ACLs.

Security group	                | Network ACL
Operates at the instance level	| Operates at the subnet level
Applies to an instance only if it is associated with the instance	 | Applies to all instances deployed in the associated subnet (providing an additional layer of defense if security group rules are too permissive)
Supports allow rules only	| Supports allow rules and deny rules
Evaluates all rules before deciding whether to allow traffic	| Evaluates rules in order, starting with the lowest numbered rule, when deciding whether to allow traffic
Stateful: Return traffic is allowed, regardless of the rules	| Stateless: Return traffic must be explicitly allowed by the rules
 

![image](https://github.com/souravs17031999/CCP-AWS-CLFC01/assets/33771969/432490b7-c98b-400e-a009-fc818b3345b9)

Strengthen your portfolio, predict risk, accelerate fraud detection, and augment advisory services all from a single destination, AWS Marketplace
Security orchestration, automation, and response (SOAR) paradigm
The AWS Knowledge Center helps answer the questions most frequently asked by AWS Support customers. 

An AWS system integrator has to play an important role in helping prospect customers/clients migrate. Specifically, the AWS system integrator role becomes vital when it comes to optimizing the work pressure on the AWS platform while dealing with things at a greater scale. An AWS system integrator role is to assist the customers of the concerned organization in terms of exploring AWS the best way.

# TECHNOLOGY 

# Technology

## methods of deploying and operating in the AWS Cloud:**
- Programmatic access, APIs: To connect programmatically to an AWS service, you use an endpoint. An endpoint is the URL of the entry point for an AWS web service. 
The AWS SDKs and the AWS Command Line Interface (AWS CLI) automatically use the default endpoint for each service in an AWS Region. But you can specify an alternate endpoint for your API requests.
- SDKs: Easily develop applications on AWS in the programming language of your choice
- AWS Management Console: Everything you need to access and manage the AWS Cloud — in one web interface
- CLI: The AWS Command Line Interface (AWS CLI) is an open source tool that enables you to interact with AWS services using commands in your command-line shell.
-  Infrastructure as Code: Practicing infrastructure as code means applying the same rigor of application code development to infrastructure provisioning. 
All configurations should be defined in a declarative way and stored in a source control system

**AWS Elastic Beanstalk
- high-level deployment tool, platform configuration defines the infrastructure and software stack to be used for a given environment.  
- Upload your code and Elastic Beanstalk automatically handles the deployment—from capacity provisioning, load balancing, and auto scaling to application health monitoring.  
- returns a production ready instance URL

![image](https://github.com/souravs17031999/CCP-AWS-CLFC01/assets/33771969/22e17316-da85-4826-b78e-31463c8012fa)   

**AWS CloudFormation
- Create a template (JSON or YAML) that describes all the AWS resources that you want. It takes care of provisioning and configuring those resources for you. This file serves as the single source of truth for your cloud environment.
![image](https://github.com/souravs17031999/CCP-AWS-CLFC01/assets/33771969/a6897c25-cec2-47d1-906c-2160cc22fff8)

Futbol Club Barcelona Case Study:   
- We are very happy with AWS CloudFormation, because it means we are able to use ‘one-click’ deployment of our whole infrastructure.AWS is really a game changer because it removes the need to devote a lot of the dedication and resources to micro-manage your own platform. AWS is key for us. As a service provider, it allows us to combine the best time to market and technical capability with very competitive prices and a pay-per-use model. Because AWS is strategic for us, we also have an AWS specialized team to provide services for our customers.  

Expedia Group:  
- From an architectural perspective, infrastructure, automation, and proximity to the customer were key factors,
- By deploying ESS on AWS, Expedia Group was able to improve service to customers in the Asia Pacific region as well as Europe. “Latency was our biggest issue,” says Chandramouli. “Using AWS, we decreased average network latency from 700 milliseconds to less than 50 milliseconds

**AWS OpsWorks
- Chef and Puppet are automation platforms that allow you to use code to automate the configurations of your servers. OpsWorks lets you use Chef and Puppet to automate how servers are configured, deployed, and managed across your Amazon EC2 instances or on-premises compute environments. 

**AWS CodeCommit
- Makes it easy for companies to host secure and highly scalable private Git repositories.
- Maintain your repositories close to your build, staging, and production environments on AWS.

**AWS  CodeBuild
- You just specify the location of your source code and choose your build settings, and CodeBuild will run your build scripts for compiling, testing, and packaging your code.
- Automate continuous integration and delivery (CI/CD) pipelines

**AWS  CodeDeploy
- Automate and consistently deploy your applications  across your development, test, and production environments.
- Support multiple deployment types, including in-place, canary, and blue/green deployments.

**AWS  CodePipeline
- AWS CodePipeline is a fully managed continuous delivery service that helps you automate your release pipelines for fast and reliable application and infrastructure updates.
- Update existing pipelines and provide templates for creating new pipelines with a declarative JSON document.

**AWS  CodeStar
- AWS CodeStar enables you to quickly develop, build, and deploy applications on AWS. AWS CodeStar provides a unified user interface, enabling you to easily manage your software development activities in one place. 
- With AWS CodeStar, you can set up your entire continuous delivery toolchain in minutes, allowing you to start releasing code faster.   

![image](https://github.com/souravs17031999/CCP-AWS-CLFC01/assets/33771969/b90c0c37-70f9-4923-848a-524836994f90)  

-----------------------------------------------------------------------------------------------------------------   
## cloud deployment models
**Hybrid :**    

**AWS Outposts: 
- family of fully managed solutions delivering AWS infrastructure and services to virtually any on-premises or edge location for a truly consistent hybrid experience.
- ex. Riot was able to rapidly deploy game servers and reduce latency by 10 to 20 milliseconds, minimizing peeker’s advantage and creating a level playing field for all players.
- AWS Partner Network (APN) delivers and installs on-premises
- Racks: Deliver low-latency compute (single digit millisecond latency), • Includes integrated networking gear.
- Servers: Run AWS Outposts in locations with limited space or smaller capacity requirements, • Does not include integrated networking gear.
- Overall, they are connected to aws using direct connect or local gateway.

**AWS Wavelength:
- Develop applications once and scale deployments to multiple Wavelength Zones across global 5G networks.
- Leverage proven AWS infrastructure and services to accelerate innovative 5G edge application development.

**AWS local zones:
- Run applications that require single-digit millisecond latency or local data processing by bringing AWS infrastructure closer to your end users and business centers.
- ex. Netflix artistic collaboration in real time , remote workstations in cloud 
- ex. Couchbase Reduces Latency by 80% for Its Distributed Database Solutions Using AWS Local Zones
- Local Zones allow customers to gain all the benefits of having compute and storage resources closer to end-users, without the need to own and operate their own data center infrastructure.

**AWS Snow:
- Move petabytes of data to and from AWS, or process data at the edge
- Purpose-built devices to cost effectively move petabytes of data, offline. Lease a Snow device to move your data to the cloud.
- encryption, AWS Snow devices feature a Trusted Platform Module (TPM) that provides a hardware root of trust. Each device is inspected after each use to ensure the integrity of the device and helps preserve the confidentiality of your data.
- Snowcone, Snowball, SnowMobile 

## connectivity options

**VPN**  
- Connect your on-premises networks and remote workers to the cloud
- Integrate with Mobile Device Management (MDM) solutions to reject devices that do not comply with your policies.
- Use Site-to-Site VPN connections to communicate securely between remote sites.

**AWS Direct Connect**   
- Create a dedicated network connection to AWS  
- Once you link your network to AWS Direct Connect, you can use SiteLink to send data between your locations. When using SiteLink, data travels over the shortest path between locations.
- The AWS Direct Connect cloud service is the shortest path to your AWS resources. While in transit, your network traffic remains on the AWS global network and never touches the public internet. This reduces the chance of hitting bottlenecks or unexpected increases in latency.   
![image](https://github.com/souravs17031999/CCP-AWS-CLFC01/assets/33771969/015268c6-5bfb-478e-9589-4388c198e63e)


**Public Internet**  
- Connect to the internet using an internet gateway
- An internet gateway is a horizontally scaled, redundant, and highly available VPC component that allows communication between your VPC and the internet. It supports IPv4 and IPv6 traffic.
------------------------------------------------------------------------------------------------------  

## AWS global infrastructure

**Regions, Availability Zones, and Edge Locations**
- physical location around the world where we cluster data centers. We call each group of logical data centers an Availability Zone. Each AWS Region consists of a minimum of three, isolated, and physically separate AZs within a geographic area.
- An Availability Zone (AZ) is one or more discrete data centers with redundant power, networking, and connectivity in an AWS Region.
- All AZs in an AWS Region are interconnected with high-bandwidth, low-latency networking, over fully redundant, dedicated metro fiber providing high-throughput, low-latency networking between AZs.
- If an application is partitioned across AZs, companies are better isolated and protected from issues such as power outages, lightning strikes, tornadoes, earthquakes, and more. AZs are physically separated by a meaningful distance, many kilometers, from any other AZ, although all are within 100 km (60 miles) of each other.
- The code for Availability Zone is its Region code followed by a letter identifier. For example, us-east-1a.
- Edge location: A site that CloudFront uses to cache copies of your content for faster delivery to users at any location.

![image](https://github.com/souravs17031999/CCP-AWS-CLFC01/assets/33771969/2f518521-737a-44e7-8a0a-fc765fa237fd)

**how to achieve high availability through the use of multiple Availability Zones**
- Single points of failure (SPOF) are commonly eliminated with an N+1 or 2N redundancy configuration, where N+1 is achieved via load balancing among active–active nodes, and 2N is achieved by a pair of nodes in active–standby configuration.
- AWS has several methods for achieving HA through both approaches, such as through a scalable, load balanced cluster or assuming an active–standby pair.
- High availability for systems is represented through a sequence of “9s”.
- Three-nine availability, represented as 99.9%, allows 8 hours and 46 minutes of downtime per year.
- Four-nine availability, 99.99%, allows 52 minutes and 36 seconds of downtime per year.
- Five-nine availability, 99.999% which is the accepted standard for mission-critical operations, provides about 5 minutes and 15 seconds of downtime per year.

**Multi-Region Application Architecture**  
![image](https://github.com/souravs17031999/CCP-AWS-CLFC01/assets/33771969/c2eb9614-7e2c-4376-8569-92fe83532969)  

**Disaster recovery options in the cloud** 

*RTO stands for Recovery Time Objective and is a measure of how quickly after an outage an application must be available again
*RPO, or Recovery Point Objective, refers to how much data loss your application can tolerate.
*Another way to think about RPO is how old can the data be when this application is recovered? With both RTO and RPO, the targets are measured in hours, minutes, or seconds, with lower numbers representing less downtime or less data loss.

RTO questions: 
What is the impact if this application is unavailable? Does this impact change over time?  
Is there a financial cost? How much per hour?  
Is there a reputational cost? E.g. – A public facing corporate landing page  
Does this application have an SLA with internal or external customers?   
Are there external compliance or regulatory requirements this application is subject to?  
Does this application depend on any other applications? Do you know of any applications that depend on this application?  

RPO questions:
What is the impact of data loss for this application?  
Is there a financial cost? How much per hour?   
Is there a reputational cost?   
Does this application have an SLA with internal or external customers?   
Are there external compliance or regulatory requirements this application is subject to?   
Do other applications that depend on data from this application? What are their RPO requirements?  
Can lost data be recreated? How long would this take, and is this acceptable?   
How often does the data change?   

**AWS Resilience Hub
- AWS Resilience Hub was recently released to help customers establish RPO/RTO targets per application, and then analyze applications against those targets. 
- Resiliency Policies are assigned to one or more applications, creating a Tier.  Applications are then assessed against their Tier’s targets either via a direct request from a user, via a scheduled assessment, or as part of your CI/CD pipeline. 
- Following this assessment, Resilience Hub provides a breakdown of what individual components within your application meet, exceed, or fall short of the targeted objective.   

![image](https://github.com/souravs17031999/CCP-AWS-CLFC01/assets/33771969/6380a8ff-93ca-4e06-9971-8357c27abdc9)   

![image](https://github.com/souravs17031999/CCP-AWS-CLFC01/assets/33771969/d3b7b169-91c7-471c-babf-893230c6943f)   

![image](https://github.com/souravs17031999/CCP-AWS-CLFC01/assets/33771969/de4837d1-bb63-4b38-b56a-54308232123c)  

![image](https://github.com/souravs17031999/CCP-AWS-CLFC01/assets/33771969/dbfe0b1b-ff58-4429-a594-4cf9aa6e2bb3)   


**backup_and_restore
- suitable approach for mitigating against data loss or corruption. This approach can also be used to mitigate against a regional disaster by replicating data to other AWS Regions  
- Your workload data will require a backup strategy that runs periodically or is continuous. How often you run your backup will determine your achievable recovery point (which should align to meet your RPO). The backup should also offer a way to restore it to the point in time in which it was taken.     

![image](https://github.com/souravs17031999/CCP-AWS-CLFC01/assets/33771969/da8cee29-f316-4c2a-a5f0-bbbd41de88ff)    


**pilot_light
- you replicate your data from one Region to another and provision a copy of your core workload infrastructure.
- Resources required to support data replication and backup, such as databases and object storage, are always on
- Other elements, such as application servers, are loaded with application code and configurations, but are "switched off" and are only used during testing or when disaster recovery failover is invoked.

![image](https://github.com/souravs17031999/CCP-AWS-CLFC01/assets/33771969/1fea0294-ea1c-4cb7-9793-e793291c2553)    

_Warm standby_
- involves ensuring that there is a scaled down, but fully functional, copy of your production environment in another Region.   
![image](https://github.com/souravs17031999/CCP-AWS-CLFC01/assets/33771969/74e5803c-6cbc-4d78-9916-c63f31c56bc8)

_Multi-site active/active_
- multi-site active/active or hot standby active/passive strategy
- Hot standby uses an active/passive configuration where users are only directed to a single region and DR regions do not take traffic.

![image](https://github.com/souravs17031999/CCP-AWS-CLFC01/assets/33771969/9dee3a81-2e23-4409-8e6f-1449b4bed785)
  
**High availability is not disaster recovery**

**Edge Locations**

**Amazon CloudFront_
- Reduce latency by delivering data through 450+ globally dispersed Points of Presence (PoPs) with automated network mapping and intelligent routing.
- AWS Shield Standard to defend against DDoS attacks at no additional charge.
- Customize the code you run at the AWS content delivery network (CDN) edge using serverless compute features
- Scale automatically to deliver software, game patches, and IoT over-the-air (OTA) updates at scale with high transfer rates.

![image](https://github.com/souravs17031999/CCP-AWS-CLFC01/assets/33771969/76d27703-ad2d-4522-9066-0c8ccf177101)   


**AWS Global Accelerator_
- Deliver highly available applications with fast failover for multi-Region and multi-AZ architectures.
- Achieve deterministic routing by removing DNS cache dependencies.
- Global Accelerator provides two global static public IPs that act as a fixed entry point to your application endpoints such as ALB etc...  
- Use traffic dials to route traffic to the nearest Region or achieve fast failover across Regions.

![image](https://github.com/souravs17031999/CCP-AWS-CLFC01/assets/33771969/9fe2e7b7-48f6-4f67-b031-2c3083097118)   


----------------------------------------------------------------------------

## AWS Compute Services

**different compute families**

![image](https://github.com/souravs17031999/CCP-AWS-CLFC01/assets/33771969/169992e3-2237-4872-9a86-f31a8151f799)   

Instance families   
C – Compute  

D – Dense storage  

F – FPGA   

G – GPU   

Hpc – High performance computing  

I – I/O   

Inf – AWS Inferentia   

M – Most scenarios   

P – GPU   

R – Random access memory  

T – Turbo   

Trn – AWS Tranium  

U – Ultra-high memory  

VT – Video transcoding   

X – Extra-large memory  

Additional capabilities    
a – AMD processors   

g – AWS Graviton processors   

i – Intel processors   

d – Instance store volumes   

n – Network optimization    

b – Block storage optimization  

e – Extra storage or memory   

z – High frequency  

_general purpose_
- provide a balance of compute, memory and networking resources, and can be used for a variety of diverse workloads.
- ex. Applications built on open-source software such as application servers, microservices, gaming servers, midsize data stores, and caching fleets.  

_Compute Optimized_
- Compute Optimized instances are ideal for compute bound applications that benefit from high performance processors. high performance computing (HPC), scientific modeling, dedicated gaming servers, machine learning etc...
- ex. batch processing, ad serving, video encoding, gaming, scientific modelling

_Memory Optimized_
- Memory optimized instances are designed to deliver fast performance for workloads that process large data sets in memory.
- ex. open-source databases, in-memory caches, and real-time big data analytics.

_Storage Optimized_
- Storage optimized instances are designed for workloads that require high, sequential read and write access to very large data sets on local storage. IOPS applications.
- ex. MySQL, MariaDB, and PostgreSQL), and NoSQL databases (KeyDB, ScyllaDB, and Cassandra).

_Accelerated Computing_
- Accelerated computing instances use hardware accelerators, or co-processors, to perform functions, such as floating point number calculations, graphics processing
- ex. Machine learning, high performance computing, computational fluid dynamics, computational finance, seismic analysis, speech recognition, autonomous vehicles, and drug discovery.

### Analytics
**Amazon Athena**
- Amazon Athena is an interactive query service that makes it easy to analyze data in Amazon S3 using standard SQL. Athena is serverless, so there is no infrastructure to manage, and you pay only for the queries that you run.
- Analyze data or build applications from an Amazon Simple Storage Service (S3) data lake and 30 data sources, including on-premises data sources or other cloud systems using SQL or Python.
- Analyze petabyte-scale data where it lives with ease and flexibility

**Amazon Kinesis**
- Collect, process, and analyze real-time video and data streams
- Build apps for application monitoring, fraud detection, and live leaderboards. Analyze data and emit the results to any data store or application.
- Perform real-time analytics on data that has been traditionally analyzed using batch processing.
- Process streaming data from IoT devices, and then use the data to programmatically send real-time alerts and respond when a sensor exceeds certain operating thresholds.

## Application integration

**Amazon SNS (Simple Notification service)**
- Fully managed Pub/Sub service
- Deliver application-to-application (A2A) notifications to integrate and decouple distributed applications.
- Distribute application-to-person (A2P) notifications to your customers with SMS texts, push notifications, and email.
- Push mechanism

**Amazon SQS (Simple Queue service)**
- Fully managed message queuing for microservices
- Separate frontend from backend systems, such as in a banking application. Customers immediately get a response, but the bill payments are processed in the background.
- Amazon SQS provides a simple and reliable way for customers to decouple and connect components (microservices) together using queues.
- Pull mechanism

## Customer Engagement
**Amazon Connect**  
- Provide superior customer service at a lower cost with an easy-to-use cloud contact center
- Set up a cloud contact center in just a few clicks and onboard agents to help customers right away.
- Improve agent productivity and customer experience across voice and digital channels with the all-in-one, AI- and ML-powered contact center.

## Management, Monitoring, and Governance:
**Amazon EventBridge**
- Easily build loosely coupled, event-driven architectures to help you deploy new features faster.  

![image](https://github.com/souravs17031999/CCP-AWS-CLFC01/assets/33771969/f8b819e7-6893-4d2d-b469-a736a38ee0b2)  

**AWS License Manager**
- License Manager makes it easier for you to manage your software licenses from vendors, such as Microsoft, SAP, Oracle, and IBM, across AWS and your on-premises environments.
- Switch between license types and automate the discovery, tracking, and reporting of existing licenses.
- Automate the distribution and activation of software entitlements and workloads across AWS accounts for end users.

**AWS Secrets Manager**
- Centrally manage the lifecycle of secrets
- AWS Secrets Manager helps you manage, retrieve, and rotate database credentials, API keys, and other secrets throughout their lifecycles.

**AWS systems manager**
- Improve visibility and control in the cloud, on premises, and at the edge.
- Shorten the time to detect and resolve operational issues.
- Automate configuration and ongoing management of your applications and resources.
- AWS Systems Manager Parameter Store: provides secure, hierarchical storage for configuration data management and secrets management. You can store data such as passwords, database strings, Amazon Machine Image (AMI) IDs, and license codes as parameter values. You can store values as plain text or encrypted data. 

### Instances (virtual machines)

**EC2 (Amazon Elastic Compute Cloud)**
- Amazon EC2 delivers secure, reliable, high-performance, and cost-effective compute infrastructure to meet demanding business needs.
- Amazon EC2 delivers the broadest for ML projects 
- 4 9's availability SLA's
- The AWS Nitro System is the underlying platform for our next generation of EC2 instances: high security

**EC2 spot instances**
- Spot Instances are available at up to a 90% discount compared to On-Demand prices.
- You can run hyperscale workloads at a significant cost savings or you can accelerate your workloads by running parallel tasks.
- You also have the option to hibernate, stop or terminate your Spot Instances when EC2 reclaims the capacity back with two-minutes of notice.
- However, Spot does not guarantee that you can keep your running instances long enough to finish your workloads. Spot also does not guarantee that you can get immediate availability of the instances that you are looking for, or that you can always get the aggregate capacity that you requested. 

**EC2 Auto Scaling**
- Add or remove compute capacity to meet changing demand
- Follow the demand curve for your applications so that you don’t have to provision Amazon EC2 capacity in advance.
- An Auto Scaling group contains a collection of EC2 instances that are treated as a logical grouping for the purposes of automatic scaling and management.
- An Auto Scaling group also lets you use Amazon EC2 Auto Scaling features such as health check replacements and scaling policies.
- scaling options: Maintain current instance levels at all times, Scale manually where you specify only the change in the maximum, minimum, or desired capacity of your Auto Scaling group, Scale based on a schedule, Scale based on demand, Use predictive scaling.

**Load Balancers**
- Elastic Load Balancing automatically distributes your incoming application traffic across all the EC2 instances that you are running. Elastic Load Balancing helps to manage incoming requests by optimally routing traffic so that no one instance is overwhelmed.
- To use Elastic Load Balancing with your Auto Scaling group, attach the load balancer to your Auto Scaling group.

_Application Load Balancer_
- Routes and load balances at the application layer (HTTP/HTTPS), and supports path-based routing. WAF rules to protect against common attacks.

_Network Load Balancer_
- Routes and load balances at the transport layer (TCP/UDP Layer-4), based on address information extracted from the Layer-4 header.

_Gateway Load Balancer_
- Distributes traffic to a fleet of appliance instances. Provides scale, availability, and simplicity for third-party virtual appliances, such as firewalls, intrusion detection and prevention systems, and other appliances.

_Classic Load Balancer_
- Routes and load balances either at the transport layer (TCP/SSL), or at the application layer (HTTP/HTTPS).

![image](https://github.com/souravs17031999/CCP-AWS-CLFC01/assets/33771969/ba33f4ca-82c2-43cc-89e8-c24d43e795cc)   

**Amazon LightSail**
- Use pre-configured development stacks like LAMP, Nginx, MEAN, and Node.js. to get online quickly and easily.
- Build and personalize your blog, ecommerce, or personal website in just a few clicks, with pre-configured applications like WordPress, Magento, Prestashop, and Joomla.
- Easily create and delete development sandboxes and test environments

**Amazon Batch**
- Run hundreds of thousands of batch and machine learning (ML) computing jobs without installing software or servers.
- AWS Batch lets developers, scientists, and engineers efficiently run hundreds of thousands of batch and ML computing jobs while optimizing compute resources
- Run financial services analyses, drugs and sequence genomes, render visual effects, ML training Jobs etc...

![image](https://github.com/souravs17031999/CCP-AWS-CLFC01/assets/33771969/14b48264-0ffc-4bf8-9071-be70015173cf)  
![image](https://github.com/souravs17031999/CCP-AWS-CLFC01/assets/33771969/2856695b-b81e-4fad-8325-18d99bdf98e6)  
![image](https://github.com/souravs17031999/CCP-AWS-CLFC01/assets/33771969/c3e5efa0-e26f-44e8-8e4a-ca1efd58014b)   

### Containers 

**Amazon Elastic Container Service (Amazon ECS)**
- Run highly secure, reliable, and scalable containers. fully managed container orchestration service
- Automatically scale and run web applications in multiple Availability Zones with the performance, scale, reliability, and availability of AWS.
- Plan, schedule, and run batch computing workloads and train NLP, ML models without managing the infrastructure by using Amazon ECS with AWS Fargate.
- Amazon ECS supports Docker and enables you to run and manage Docker containers.
- Amazon ECS allows you to define tasks through a JavaScript Object Notation (JSON) template called a _Task Definition_. Within a Task Definition, you can specify one or more containers that are required for your task, including the Docker repository and image, memory and CPU requirements, shared data volumes, and how the containers are linked to each other.You can launch as many tasks as you want from a single Task Definition file that you can register with the service.

**Amazon ECS Anywhere**
- Run containers on your on-premises infrastructure
- Run containerized data-processing workloads at edge locations on your own hardware to maintain reduced latency.
- Use your existing Windows Server licenses to run Windows container workloads in on-premises environments.

**Amazon Elastic Container Registry**
- Push container images to Amazon ECR without installing or scaling infrastructure, and pull images using any management tool.
- Meet your image compliance security requirements using the tightly integrated Amazon Inspector vulnerability management service
- Access and distribute your images faster, reduce download times, and improve availability using a scalable, durable architecture.
- like Docker Hub etc...

![image](https://github.com/souravs17031999/CCP-AWS-CLFC01/assets/33771969/e477eb63-f888-4984-a638-c5a482bd4bad)  

**Amazon Elastic Kubernetes Service (Amazon EKS)**
- The most trusted way to start, run, and scale Kubernetes. Leverage built-in integrations with AWS services such as EC2, VPC, IAM, EBS and more 
- Ensure a more secure Kubernetes environment with security patches automatically applied to your cluster’s control plane.

**AWS Fargate**
- Deploy and manage your applications, not infrastructure. Fargate removes the operational overhead of scaling, patching, securing, and managing servers.
- AWS Fargate is compatible with both Amazon Elastic Container Service (ECS) and Amazon Elastic Kubernetes Service (EKS).
- abstracts the underlying infrastructure and can be used to launch and run containers without having to provision or manage EC2 instances.

**App Runner**
- AWS App Runner is a fully managed container application service that lets you build, deploy, and run containerized web applications and API services without prior infrastructure or container experience.

![image](https://github.com/souravs17031999/CCP-AWS-CLFC01/assets/33771969/95368ddc-5a1e-475f-afdd-ac910e4ad3ea)  

### Serverless

**AWS Lambda**
- Run code without provisioning or managing infrastructure. Simply write and upload code as a .zip file or container image.
- Automatically respond to code execution requests at any scale, from a dozen events per day to hundreds of thousands per second.
- Respond to high demand in double-digit milliseconds with Provisioned Concurrency.

![image](https://github.com/souravs17031999/CCP-AWS-CLFC01/assets/33771969/797c8230-a472-4c72-8621-3322f79d1e90)  
![image](https://github.com/souravs17031999/CCP-AWS-CLFC01/assets/33771969/f717d5da-84d7-4ca8-b4c0-d840c37e2ccc)  
![image](https://github.com/souravs17031999/CCP-AWS-CLFC01/assets/33771969/ec2dd29a-cefc-4722-8a92-ee6afdd76b98)  
![image](https://github.com/souravs17031999/CCP-AWS-CLFC01/assets/33771969/40434a2a-9068-4d16-9875-10bd21ee426e)  
![image](https://github.com/souravs17031999/CCP-AWS-CLFC01/assets/33771969/8260f2f7-a30a-4b58-805c-3b3659e5bbef)  

**Amazon WorkSpaces**
- Amazon WorkSpaces is a fully managed desktop virtualization service for Windows, Linux, and Ubuntu, that allows you to access resources from any supported device.  
- Facilitate remote work, learning environments for students and educators   
- Allow devs to test quickly their code across multiple environments 

### Edge and Hybrid

- All covered earlier 

### Cost management

**EC2 Image Builder**
- EC2 Image Builder simplifies the building, testing, and deployment of Virtual Machine and container images for use on AWS or on-premises.
- providing a simple graphical interface, built-in automation, and AWS-provided security settings. With Image Builder, there are no manual steps for updating an image nor do you have to build your own automation pipeline.
- validate your images for functionality, compatibility, and security compliance with AWS-provided tests and your own tests before using them in production
- Examples of secure image with AWS-provided and/or custom templates includes: 1/ Ensure security patches are applied, 2/ Enforce strong passwords, 3/ Turn on full disk encryption, 4/ Close all non-essential open ports, 5/ Enable software firewall, 6/ Enable logging/audit controls.

**AMI**
- An Amazon Machine Image (AMI) is a supported and maintained image provided by AWS that provides the information required to launch an instance. You must specify an AMI when you launch an instance.
- It can be customized and published, registered or deregistered on Marketplaces open sourced.

## AWS Storage Services

![image](https://github.com/souravs17031999/CCP-AWS-CLFC01/assets/33771969/5841dd11-1331-4380-bbcf-c3671c51d426)  


**S3 (Amazon Simple Storage Service)**
- Scale storage resources to meet fluctuating needs with 99.999999999% (11 9s) of data durability.
- Protect your data with unmatched security, compliance, and audit capabilities.
- object storage service offering industry-leading scalability, data availability, security, and performance.
- Amazon S3 is the best place to build data lakes because of its unmatched durability, availability, scalability, security, compliance, and audit capabilities.
- provides strong read-after-write consistency for PUT and DELETE requests of objects in your Amazon S3 bucket in all AWS Regions.

_S3 storage classes_
![image](https://github.com/souravs17031999/CCP-AWS-CLFC01/assets/33771969/9080c265-6b53-414d-9d60-02c8e94fb92b)   

_storage management_
- S3 Lifecycle – Configure a lifecycle configuration to manage your objects and store them cost effectively throughout their lifecycle.
- S3 Object Lock – Prevent Amazon S3 objects from being deleted or overwritten for a fixed amount of time or indefinitely.
- S3 Replication – Replicate objects and their respective metadata and object tags to one or more destination buckets
- S3 Batch Operations – Manage billions of objects at scale with a single S3 API request or a few clicks in the Amazon S3 console

_How it works_
- An object is a file and any metadata that describes the file. A bucket is a container for objects.
- To store your data in Amazon S3, you first create a bucket and specify a bucket name and AWS Region. Then, you upload your data to that bucket as objects in Amazon S3. Each object has a key (or key name), which is the unique identifier for the object within the bucket.
- Every object is contained in a bucket. For example, if the object named photos/puppy.jpg is stored in the DOC-EXAMPLE-BUCKET bucket in the US West (Oregon) Region, then it is addressable by using the URL https://DOC-EXAMPLE-BUCKET.s3.us-west-2.amazonaws.com/photos/puppy.jpg   
DOC-EXAMPLE-BUCKET is the name of the bucket and photos/puppy.jpg is the key.
- can have up to 100 buckets in your account
- Versioning in Amazon S3 is a means of keeping multiple variants of an object in the same bucket. You can use the S3 Versioning feature to preserve, retrieve, and restore every version of every object stored in your buckets.
- Individual Amazon S3 objects can range in size from a minimum of 0 bytes to a maximum of 5 TB. The largest object that can be uploaded in a single PUT is 5 GB.
- The total volume of data and number of objects you can store in Amazon S3 are unlimited.

**Amazon S3 Glacier**
- Amazon S3 Glacier (S3 Glacier) is a secure and durable service for low-cost data archiving and long-term backup.
- S3 Glacier is a REST-based web service. In terms of REST, vaults and archives are the resources.
- In S3 Glacier, a vault is a container for storing archives. A vault is similar to an Amazon S3 bucket.
- An archive can be any data, such as a photo, video, or document. An archive is similar to an Amazon S3 object, and is the base unit of storage in S3 Glacier.
- ex. https://region-specific-endpoint/account-id/vaults/vault-name/archives/archive-id
- Retrieving archives and vault inventories (lists of archives) are asynchronous operations in S3 Glacier, in which you first initiate a job, and then download the job output after S3 Glacier completes the job.
- Because jobs take time to run, S3 Glacier supports a notification mechanism to notify you when a job is completed. You can configure a vault to send a notification to an Amazon Simple Notification Service (Amazon SNS) topic when a job is completed.

![image](https://github.com/souravs17031999/CCP-AWS-CLFC01/assets/33771969/1f2ac5eb-a45d-4c50-b56f-c288787088de)  
![image](https://github.com/souravs17031999/CCP-AWS-CLFC01/assets/33771969/95ab3c3b-df06-4e86-a16d-e65a86fcbf53)   
![image](https://github.com/souravs17031999/CCP-AWS-CLFC01/assets/33771969/4f22ebd5-344e-4a83-9358-63192bad49e7)    

**Amazon Elastic Block Store (Amazon EBS)**
- block level storage volumes for use with EC2 instances. EBS volumes behave like raw, unformatted block devices. You can mount these volumes as devices on your instances.
- EBS volumes that are attached to an instance are exposed as storage volumes that persist independently from the life of the instance.
- EBS volumes are particularly well-suited for use as the primary storage for file systems, databases, or for any applications that require fine granular updates and access to raw, unformatted, block-level storage.
- Block storage provides low latency and high-performance values in various use cases. Its features are primarily useful for structured database storage, VM file system volumes, and high volumes of read and write loads. Object storage is best used for large amounts of unstructured data, especially when durability, unlimited storage, scalability, and complex metadata management are relevant factors for overall performance.
- Run relational or NoSQL databases: Oracle, Microsoft SQL Server, PostgreSQL, MySQL, Cassandra, and MongoDB etc...
- Right-size your big data analytics engines : Hadoop, Spark etc..
- Attach high-performance and high-availability block storage for mission-critical applications, on-premises storage area network (SAN workloads)
- EBS volumes: SSD, HDD etc....
- EBS snapshots:  You can back up the data on your Amazon EBS volumes to Amazon S3 by taking point-in-time snapshots. Snapshots are incremental backups, which means that only the blocks on the device that have changed after your most recent snapshot are saved.
- 
![image](https://github.com/souravs17031999/CCP-AWS-CLFC01/assets/33771969/be64b5b0-ebc5-4569-b3c5-8d736654e7e1)  

**Amazon Elastic File System (EFS)**
- Amazon Elastic File System (EFS) automatically grows and shrinks as you add and remove files with no need for management or provisioning.
- managed file system designed for 99.999999999 percent (11 9s) durability and up to 99.99 percent (4 9s) of availability.
- Simplify persistent storage for modern content management system (CMS) workloads.
- Persist and share data from your AWS containers and serverless applications with zero
management required.
- Powerful combination of lambda serverless with EFS.
- Stateful microservices, allowing data to persist application state.
- Amazon Elastic File Service (EFS) is the solution you can use to provide users with NFS capabilities.

**Amazon Storage Gateway**
- Store and access objects in Amazon S3 from NFS or SMB file data with local caching    
- File gateway, FsX file gateway, Tape gateway, Volume gateway    

![image](https://github.com/souravs17031999/CCP-AWS-CLFC01/assets/33771969/69197bb9-8c7c-4b14-99ce-b27a45e1f21b)   

![image](https://github.com/souravs17031999/CCP-AWS-CLFC01/assets/33771969/0438ce41-073c-407a-875f-3c7e7d35b6cd)   

![image](https://github.com/souravs17031999/CCP-AWS-CLFC01/assets/33771969/5e47f302-631d-4fa2-ad18-7d8fae42ac84)   

## AWS Networking Services   

**VPC (Amazon Virtual Private Cloud)**

![image](https://github.com/souravs17031999/CCP-AWS-CLFC01/assets/33771969/c991b728-6052-4410-8f14-94104e24374b)    

- you can launch AWS resources in a logically isolated virtual network that you've defined.  
- A subnet is a range of IP addresses in your VPC. A subnet must reside in a single Availability Zone.
- Each subnet must be associated with a route table, which specifies the allowed routes for outbound traffic leaving the subnet.  
- You can assign IP addresses, both IPv4 and IPv6, to your VPCs and subnets.
- Use route tables to determine where network traffic from your subnet or gateway is directed.
- A gateway connects your VPC to another network. For example, use an internet gateway to connect your VPC to the internet. Use a VPC endpoint to connect to AWS services privately   
- An internet gateway enables resources in your public subnets (such as EC2 instances) to connect to the internet if the resource has a public IPv4 address or an IPv6 address. Similarly, resources on the internet can initiate a connection to resources in your subnet using the public IPv4 address or IPv6 address.  
- Use a VPC peering connection to route traffic between the resources in two VPCs.
_VPC PEERING CONNECTION_  
![image](https://github.com/souravs17031999/CCP-AWS-CLFC01/assets/33771969/4a888b9d-eecc-4b6e-9039-95eebcfb89de)     

- Use a transit gateway, which acts as a central hub, to route traffic between your VPCs, VPN connections, and AWS Direct Connect connections
_AWS TRANSIT GATEWAY_  
![image](https://github.com/souravs17031999/CCP-AWS-CLFC01/assets/33771969/d5f18a11-1c7e-4f66-99d9-2660c972ac85)   

- An Elastic IP address is a static, public IPv4 address designed for dynamic cloud computing. You can associate an Elastic IP address with any instance or network interface in any VPC in your account.
- A NAT gateway is a Network Address Translation (NAT) service. You can use a NAT gateway so that instances in a private subnet can connect to services outside your VPC but external services cannot initiate a connection with those instances. Each NAT gateway is created in a specific Availability Zone.
- Internet traffic from the instances in the private subnet is routed to the NAT instance, which then communicates with the internet. Therefore, the NAT instance must have internet access. It must be in a public subnet (a subnet that has a route table with a route to the internet gateway), and it must have a public IP address or an Elastic IP address.
![image](https://github.com/souravs17031999/CCP-AWS-CLFC01/assets/33771969/bd27b072-7ad2-4e5c-8a6c-fdd12b40e57a)    

_Subnet types_
- Public subnet – The subnet has a direct route to an internet gateway. Resources in a public subnet can access the public internet.
- Private subnet – The subnet does not have a direct route to an internet gateway. Resources in a private subnet require a NAT device to access the public internet.
- The subnet has a route to a Site-to-Site VPN connection through a virtual private gateway. The subnet does not have a route to an internet gateway.
- Isolated subnet – The subnet has no routes to destinations outside its VPC. Resources in an isolated subnet can only access or be accessed by other resources in the same VPC.

![image](https://github.com/souravs17031999/CCP-AWS-CLFC01/assets/33771969/e0f049ac-e9b3-418b-959c-4c7841161588)   

**Security Groups**
- A security group controls the traffic that is allowed to reach and leave the resources that it is associated with. 
- A security group acts as a firewall that controls the traffic allowed to and from the resources in your virtual private cloud (VPC). You can choose the ports and protocols to allow for inbound traffic and for outbound traffic.  
- For example, after you associate a security group with an EC2 instance, it controls the inbound and outbound traffic for the instance.  
- The following table summarizes the basic differences between security groups and network ACLs.    
- When you first create a security group, it has no inbound rules, it has an outbound rule that allows all outbound traffic from the resource.   

_Network ACL_
- A network access control list (ACL) allows or denies specific inbound or outbound traffic at the subnet level.
-  By default, it allows all inbound and outbound IPv4 traffic and, if applicable, IPv6 traffic.

Security group	Network ACL   
Operates at the instance level	Operates at the subnet level  
Applies to an instance only if it is associated with the instance	Applies to all instances deployed in the associated subnet (providing an additional layer of defense if security group rules are too permissive)  
Supports allow rules only	Supports allow rules and deny rules  
Evaluates all rules before deciding whether to allow traffic	Evaluates rules in order, starting with the lowest numbered rule, when deciding whether to allow traffic  
Stateful: Return traffic is allowed, regardless of the rules	Stateless: Return traffic must be explicitly allowed by the rules  

![image](https://github.com/souravs17031999/CCP-AWS-CLFC01/assets/33771969/3a2718ee-be49-4969-9b35-03740e1b747f)   

**Route 53**

![image](https://github.com/souravs17031999/CCP-AWS-CLFC01/assets/33771969/50941470-5dd2-4018-abdd-f630b93819e0)     

- Amazon Route 53 is a highly available and scalable Domain Name System (DNS) web service.
_In the following order:_ 
- Register domain names: Route 53 lets you register a name for your website or web application, known as a domain name.
- Route internet traffic to the resources for your domain: When a user opens a web browser and enters your domain name (example.com) or subdomain name (acme.example.com) in the address bar, Route 53 helps connect the browser with your website or web application.
- Check the health of your resources: Route 53 sends automated requests over the internet to a resource, such as a web server, to verify that it's reachable, available, and functional.   


- top-level domain (TLD): The last part of a domain name, such as .com, .org, or .ninja.
- The Domain Name System translates easily understood names such as example.com into the numbers, known as IP addresses, that allow computers to find each other on the internet.
- DNS resolver: A DNS server, often managed by an internet service provider (ISP), that acts as an intermediary between user requests and DNS name servers.
- A CIDR block is an IP range used with IP-based routing. In Route 53 You can specify CIDR block from /0 to /24 for IPv4 and/0 to /48 for IPv6.
- DNS record: An object in a hosted zone that you use to define how you want to route traffic for the domain or a subdomain. For example, you might create records for example.com and www.example.com that route traffic to a web server that has an IP address of 192.0.2.234.
- TTL (time to live): The amount of time, in seconds, that you want a DNS resolver to cache (store) the values for a record before submitting another request to Route 53  
_Multiple routing policy:_

Simple routing policy – Use to route internet traffic to a single resource that performs a given function for your domain, for example, a web server that serves content for the example.com website.  

Failover routing policy – Use when you want to configure active-passive failover.   

Geolocation routing policy – Use when you want to route internet traffic to your resources based on the location of your users.  

Geoproximity routing policy – Use when you want to route traffic based on the location of your resources and, optionally, shift traffic from resources in one location to resources in another.   

Latency routing policy – Use when you have resources in multiple locations and you want to route traffic to the resource that provides the best latency.   

IP-based routing policy – Use when you want to route traffic based on the location of your users, and have the IP addresses that the traffic originates from.   

Multivalue answer routing policy – Use when you want Route 53 to respond to DNS queries with up to eight healthy records selected at random.    

Weighted routing policy – Use to route traffic to multiple resources in proportions that you specify.    

**Amazon API Gateway**
- APIs act as the "front door" for applications to access data, business logic, or functionality from your backend services.
- fully managed service that makes it easy for developers to create, publish, maintain, monitor, and secure APIs at any scale
- you can create RESTful APIs and WebSocket APIs that enable real-time two-way communication applications.

## AWS Databases Services

- AWS Database Migration Service (AWS DMS) is a managed migration and replication service that helps move your database and analytics workloads to AWS quickly, securely, and with minimal downtime and zero data loss.

**INSTALLED DATABASES ON EC2 VS MANAGED DATABASES**
- Amazon RDS enables you to run a fully featured relational database while offloading database administration. Whereas, for more control and flexibility, EC2 will be better for your relational database.
- The entire process of configuration, management, maintenance, and security is automated by AWS. RDS is easy to set up, cost-effective and allows you to focus on more important tasks.
- EC2 gives you full control over your database, OS and software stack. It allows you to hire your own database administrators. 
- Security (data at Rest, and at transit), Availibility, automated backups (snapshots), scalibility and performance comes out of the box with RDS, with EC2 everything has to be configured.
- Compatibility: RDS supports Aurora, SQL Server, MySQL, MariaDB, PostgreSQL, and Oracle. With EC2, you can configure any database you want.
- Preferred for EC2 reasons: More control on the configuration, High Performance: It allows you to exceed your maximum database size and performance needs.

**RDS**
- Deploy and scale the relational database engines of your choice in the cloud or on-premises.
- Achieve high availability with Amazon RDS Multi-AZ deployments.
- Amazon RDS database instances are preconfigured with parameters and settings appropriate for the engine and class you have selected.
- In a few steps, Blue/Green Deployments create a staging environment that mirrors the production environment and keeps the two environments in sync using logical replication
- The AWS Nitro System makes RDS Optimized Writes possible.
- When your RDS for MySQL database uses RDS Optimized Writes, it can achieve up to two times higher write transaction throughput.
- An RDS for MariaDB DB instance that uses RDS Optimized Reads can achieve up to 2x faster query processing compared to a DB instance that doesn't use it.
- A DB instance is an isolated database environment running in the cloud. It is the basic building block of Amazon RDS. A DB instance can contain multiple user-created databases.
- Blue/green deployment: A blue/green deployment creates a staging environment that copies the production environment. In a blue/green deployment, the blue environment is the current production environment. The green environment is the staging environment. The staging environment stays in sync with the current production environment using logical replication.

![image](https://github.com/souravs17031999/CCP-AWS-CLFC01/assets/33771969/519b7914-cd76-40fb-a179-0353dc95f7cc)  
- Read Replicas make it easier to elastically scale out beyond the capacity constraints of a single DB instance for read-heavy database workloads. 
- By default, Amazon RDS creates and saves automated backups of your DB instance securely in Amazon S3 for a user-specified retention period.
- In an Amazon RDS Multi-AZ deployment, Amazon RDS automatically creates a primary database (DB) instance and synchronously replicates the data to an instance in a different AZ. When it detects a failure, Amazon RDS automatically fails over to a standby instance without manual intervention.
- Multi-Az: Two types: one standby replica, two standby replicas 

![image](https://github.com/souravs17031999/CCP-AWS-CLFC01/assets/33771969/e91d3802-8813-4296-bbac-aa524c546877)    

- Amazon RDS allows you to encrypt your databases using keys you manage through AWS Key Management Service (KMS). Amazon RDS supports the use of SSL to secure data in transit.
- Amazon RDS provides Amazon CloudWatch metrics for your database instances.


**Amazon Aurora
- Power performance-intensive applications and critical workloads while maintaining full compatibility with MySQL and PostgreSQL
- Aurora gives you the performance and availability of commercial-grade databases at one-tenth the cost.
- 5x faster than mysql, 3x faster than postgresql
- Amazon Aurora provides built-in security, continuous backups, serverless compute, up to 15 read replicas, automated multi-Region replication  

**DYNAMODB**
- Fast, flexible NoSQL database service for single-digit millisecond performance at any scale
- Create data schemas and tables in DynamoDB using sample data model templates and datasets available in NoSQL Workbench.
- Use PartiQL, a SQL-compatible query language, to query, insert, update, and delete table data in DynamoDB.
- DynamoDB supports both key-value and document data models. This enables DynamoDB to have a flexible schema, so each row can have any number of columns at any point in time.
- DynamoDB Accelerator (DAX) is an in-memory cache that delivers fast read performance: taking the time required for reads from milliseconds to microseconds
- DynamoDB provides native, server-side support for transactions
- DynamoDB supports high-traffic, extreme-scaled events and can handle millions of queries per second.

**REDSHIFT**
- Best price-performance for cloud data warehousing
- Amazon Redshift uses SQL to analyze structured and semi-structured data across data warehouses, operational databases, and data lakes
- Run and scale analytics in seconds on all your data without having to manage your data warehouse infrastructure.
- Query editor to run queries and share it across team members 
- Build insight-driven reports and dashboards using Amazon QuickSight, Tableau, Microsoft PowerBI, or other business intelligence tools.
- Amazon Redshift also is optimized for high-performance batch analysis and reporting of datasets.
- Cluster – The core infrastructure component of an Amazon Redshift data warehouse is a cluster. A cluster is composed of one or more compute nodes. The compute nodes run the compiled code. he leader node handles external communication with applications, such as business intelligence tools and query editors.
- Database – A cluster contains one or more databases. User data is stored in one or more databases on the compute nodes. Your SQL client communicates with the leader node, which in turn coordinates running queries with the compute nodes.

_data ingestion layer_: different types of data sources continuously upload structured, semistructured, or unstructured data to the data storage layer.
_data processing layer_: the source data goes through preprocessing, validation, and transformation using extract, transform, load (ETL) or extract, load, transform (ELT) pipelines
_data consumption layer_: data is loaded into your Amazon Redshift cluster, where you can run analytical workloads.  

**Amazon ElastiCache** 
- Realize microsecond response times across hundreds of millions of operations per second and up to 1 pebibyte of data
- Achieve cost-optimized performance by adding a cache for frequently read data to optimize resources and lower total cost of ownership
- Build applications quickly using popular open-source technologies, Redis and Memcached  

----------------------------------------------------------------------------------------------------------------------------  

## technology support

- there is documentation (best practices, whitepapers, AWS Knowledge Center, forums, blogs
- includes AWS Official Knowledge Center articles and videos covering the most frequent questions and requests that we receive from AWS customers.

**AWS Support**   
- The AWS Trust & Safety team can assist you when AWS resources are implicated in the following abuse types:
- Web content/non-copyright intellectual property that's objectionable content hosted on an AWS resource
- Email abuse that's an abusive email sent from an AWS resource or spam content hosted on, or sent from an AWS resource.
- Network activity that's originating from an AWS resource that causes problematic network traffic. This can be an intrusion attempt, denial-of-service (DoS) attack, port scanning, or any other abusive network activity.
- Copyright that's subject to content removal requests regarding the Digital Millennium Copyright Act.

In the AWS Management Console, you can create three types of customer cases in AWS Support:
- Account and billing
- Service limit increase requests are available to all AWS customers
- Technical support cases connect you to technical support for help with service-related technical issues 

**AWS premium support**:   
- A Technical Account Manager (TAM) provides consultative architectural and operational guidance delivered in the context of your applications and use-cases to help you achieve the greatest value from AWS. The TAM will work with you to provide tailored engagements including strategic Business Reviews, Security Improvement Programs, guided Well-Architected reviews, Cost Optimization workshops, and a range of proactive services.
- Get expert guidance and assistance achieving your objectives

**Types of Support plans**:   
- Developer: Recommended if you are experimenting or testing in AWS. Business hours** web access to Cloud Support Associates. System impaired: < 12 hours**
- Business: Minimum recommended tier if you have production workloads in AWS. 24/7 phone, web, and chat access to Cloud Support Engineers. Production system down: < 1 hour
- Enterprise on-ramp: Recommended if you have production and/or business critical workloads in AWS. 24/7 phone, web, and chat access to Cloud Support Engineers. Business-critical system down: < 30 minutes. Technical Account Management	
- Enterprise: Recommended if you have business and/or mission critical workloads in AWS. 24/7 phone, web, and chat access to Cloud Support Engineers. Business/Mission-critical system down: < 15 minutes. Access to AWS Incident Detection and Response for an additional fee. Technical Account Management	

**AWS Partner network**   
- The AWS Partner Network (APN) is a global community of partners that leverages programs, expertise, and resources to build, market, and sell customer offerings.
- As an AWS Partner, you are uniquely positioned to help customers take full advantage of all that AWS has to offer and accelerate their journey to the cloud.
- Independent Software Vendors (ISVs) and Channel Partners can take advantage of AWS Marketplace Channel features and enablement to build a scalable Marketplace business with AWS
- AWS Marketplace is a curated digital catalog that customers can use to find, buy, deploy, and manage third-party software, data, and services to build solutions and run their businesses
- You can use AWS Marketplace as a buyer (subscriber), seller (provider), or both.

**AWS Trusted Advisor**   
- AWS Trusted Advisor provides recommendations that help you follow AWS best practices.  
- These checks identify ways to optimize your AWS infrastructure, improve security and performance, reduce costs, and monitor service quotas
- AWS Basic Support and AWS Developer Support customers can access core security checks and checks for service quotas. AWS Business Support and AWS Enterprise Support customers can access all checks, including cost optimization, security, fault tolerance, performance, and service quotas.
- 
![image](https://github.com/souravs17031999/CCP-AWS-CLFC01/assets/33771969/b386ad36-ad21-4b88-b1f4-15c3968f9c71)   


# BILLING   

#  Billing and Pricing

## Pricing models 

**On-Demand**  
- On-Demand Instances let you pay for compute capacity by the hour or second with no long-term commitments. 
- recommended for 
- Users that prefer the low cost and flexibility of EC2 without any upfront payment or long-term commitment
- Applications with short-term, spiky, or unpredictable workloads that cannot be interrupted
- Applications being developed or tested on EC2 for the first time

**Amazon EC2 Spot Instances**  
- Amazon EC2 Spot Instances let you take advantage of unused EC2 capacity in the AWS cloud and are available at a discount of up to 90% compared to On-Demand prices.
- recommended for 
- Fault tolerant or stateless workloads
- Applications that can run on heterogeneous hardware
- Applications that have flexible start and end times

**reserved capacity**
- Capacity Reservations enable you to reserve compute capacity for your EC2 instances in a specific Availability Zone for any duration. Capacity reservations mitigate against the risk of being unable to get On-Demand capacity in case of capacity constraints and ensure that you always have access to EC2 capacity when you need it, for as long as you need it.
- recommended for:
- Business-critical events or workloads that require capacity assurance
- Workloads that need to meet regulatory requirements for high availability
- Disaster recovery

**Per-second billing**   
- EC2 per-second billing removes the cost of unused minutes and seconds from your bill. Focus on improving your applications instead of maximizing hourly usage, especially for instances running over irregular time periods such as dev/testing, data processing, analytics, batch processing, and gaming applications.

**Reserved instances (Savings Plans)**  
- Reserved Instances are not physical instances, but rather a billing discount that is applied to the running On-Demand Instances in your account.
-  flexible pricing model that can help you reduce your bill by up to 72% compared to On-Demand prices, in exchange for a commitment to a consistent amount of usage (measured in $/hour) for a 1- or 3-year term.
-  recommended for:
- Committed and steady-state usage
- Users looking to innovate faster by using the newest instance families, generations, and Regions while continuing to save
- All Upfront, Partial Upfront, No upfront (requires billing history)
- When you purchase a Reserved Instance, you determine the scope of the Reserved Instance. The scope is either regional or zonal.

_Types of reserved instances_: 

- Standard and Convertible: 
- you can't exchange a Standard Reserved Instance. You can exchange Convertible Reserved Instances.
- You can modify Standard and Convertible Reserved Instances.
- A Standard Reserved Instance provides a more significant discount than a Convertible Reserved Instance

- Your RI applies to any instance in its family of its same size or smaller. For example, if you purchase an RI for an m3.large instance, the RI also applies to m3.medium instances.
- Multiple smaller RIs combine to apply to larger instances. For example, if you purchase two RIs for two m3.medium instances, these RIs also apply to an m3.large instance.
- RI discounts apply to accounts in an organization's consolidated billing family depending upon whether RI sharing is turned on or off for the accounts   
If RI sharing is turned on for an account in an organization, then:
- At least one account in the organization's consolidated billing family must be running an instance that matches the specifications of the RI for the discount to apply to the management account's bill.
- The discount for any RIs purchased on that account is applied to the combined usage for that instance type on the management account's bill.
- The RI discount applies to the total usage of the instance type for the full duration of the RI. The RI discount applies even if the purchasing member account is closed.
- The discount on the RIs purchased on a member account doesn't apply to the consolidated bill if the member account leaves the organization or is removed from the organization by the management account.
- The account that originally purchased the RI receives the discount first. If the purchasing account has no instances that match the terms of the RI, then the discount for the RI is assigned to any matching usage on another account in the organization.

If RI sharing is turned off for an account in an organization, then:
- RI discounts apply only to the account that purchased the RIs.
- RI discounts from other accounts in the organization's consolidated billing family don't apply.
- The charges accrued on that account are still added to the organization's consolidated bill and are paid by the management account.

**various account structures in relation to AWS billing and pricing**   

_Consolidated billing for AWS Organizations_ 
- You can use the consolidated billing feature in AWS Organizations to consolidate billing and payment for multiple AWS accounts or multiple Amazon Web Services India Private Limited (AWS India) accounts
- Every organization in AWS Organizations has a management account that pays the charges of all the member accounts.
- One bill, Easy tracking, Combined usage
- Volume discounts: For billing purposes, AWS treats all of the accounts in the organization as if they were one account. Some services, such as AWS Data Transfer and Amazon S3, have volume pricing tiers across certain usage dimensions that give you lower prices the more you use the service. 

**Benefits of multiple accounts**   
- you can align the ownership and decision making with those accounts and avoid dependencies and conflicts with how workloads in other accounts are secured and managed.
- Workloads often have distinct security profiles that require separate control policies and mechanisms to support them.
- When you limit sensitive data stores to an account that is built to manage it, you can more easily constrain the number of people and processes that can access and manage the data store. 
- If an issue occurs within one account, impacts to workloads contained in other accounts can be either reduced or eliminated.
- using different accounts for different business units and groups of workloads can help you more easily report, control, forecast, and budget your cloud expenditures.
- When you require fine-grained cost allocation, you can apply cost allocation tags to individual resources in each of your accounts.

![image](https://github.com/souravs17031999/CCP-AWS-CLFC01/assets/33771969/5f064a35-ead7-4efa-87e2-3a0221fffe3c)  

**Billing support**   

_AWS Price List API_
- This API provides you with access to prices in JSON and CSV form. You can download and process this information on an as-needed basis. You can also elect to receive notification via Amazon Simple Notification Service (Amazon SNS) each time we make a price change.

_AWS Pricing Calculator_
- web-based planning tool that you can use to create estimates for your AWS use cases. You can use it to model your solutions before building them, explore the AWS service price points, and review the calculations behind your estimates.
- Use groups to organize services together. You can add one or more services to each group. You can also use groups to organize your estimate in different ways. For example, you can organize your estimate by cost center, service stack, product architecture, or client.

_Product_pages_
- The Products page shows the applications, tools, and cloud resources that your administrator assigned to you. You can use the Products page to launch an instance of those products.

_AWS cost explorer_
- AWS Cost Explorer is a tool that enables you to view and analyze your costs and usage. You can explore your usage and costs using the main graph, the Cost Explorer cost and usage reports, or the Cost Explorer RI reports.
- forecast how much you're likely to spend for the next 12 months, and get recommendations for what Reserved Instances to purchase
- Identify trends in expenditure 

_AWS budgets_
- You can use AWS Budgets to track and take action on your AWS costs and usage. You can use AWS Budgets to monitor your aggregate utilization and coverage metrics for your Reserved Instances (RIs) or Savings Plans.

_AWS Cost and Usage reports (CUR)_
- The AWS Cost and Usage Reports (AWS CUR) contains the most comprehensive set of cost and usage data available. You can use Cost and Usage Reports to publish your AWS billing reports to an Amazon Simple Storage Service (Amazon S3) bucket that you own.
- AWS Cost and Usage Reports tracks your AWS usage and provides estimated charges associated with your account.

_Amazon QuickSight_
- Amazon QuickSight is a cloud-scale business intelligence (BI) service that you can use to deliver easy-to-understand insights to the people who you work with, wherever they are.
- In a single data dashboard, QuickSight can include AWS data, third-party data, big data, spreadsheet data, SaaS data, B2B data, and more.
- Saves you time and money with automated and customizable data insights, powered by machine learning (ML).   
![image](https://github.com/souravs17031999/CCP-AWS-CLFC01/assets/33771969/04139eb5-a7a0-4dc9-908d-32ed8b8492b5)  

_AWS Managed Service Provider (MSP) Partner?_
- AWS Managed Service Providers (MSP) provide end-to-end AWS solutions to customers at any stage of the cloud journey—from consultation on initial solution design, to building applications, through to ongoing optimization and support.  
![image](https://github.com/souravs17031999/CCP-AWS-CLFC01/assets/33771969/fd6d4ddf-18f7-40ce-8aec-71520373bfcc)  

**Cost Allocation Via Tagging**  
- A tag is a label that you or AWS assigns to an AWS resource. Each tag consists of a key and a value.
-  You can use tags to organize your resources, and cost allocation tags to track your AWS costs on a detailed level.
-  The user-defined tags use the user prefix, and the AWS-generated tag uses the aws: prefix.

![image](https://github.com/souravs17031999/CCP-AWS-CLFC01/assets/33771969/226021a1-fdb1-41f9-8a90-6a1cc85dc0d9)   

- You can apply tags that represent business categories (such as cost centers, application names, or owners) to organize your costs across multiple services.

![image](https://github.com/souravs17031999/CCP-AWS-CLFC01/assets/33771969/82649295-4572-4fc5-bc6a-3ce5ef53dc48)  

**Billing Alarms**
- You can monitor your estimated AWS charges by using Amazon CloudWatch. When you enable the monitoring of estimated charges for your AWS account, the estimated charges are calculated and sent several times daily to CloudWatch as metric data.
- The alarm triggers when your account billing exceeds the threshold you specify. It triggers only when the current billing exceeds the threshold. It doesn't use projections based on your usage so far in the month.
- If you create a billing alarm at a time when your charges have already exceeded the threshold, the alarm goes to the ALARM state immediately.


